#!/usr/bin/env groovy

/**
 * Ansible Airgap Setup Jenkinsfile
 * Based on Jenkinsfile.recurring but adapted for airgap RKE2 infrastructure setup
 *
 * This pipeline sets up airgap RKE2 infrastructure using Ansible and OpenTofu
 * with enhanced error handling and proper workspace management.
 *
 * The pipeline uses the ANSIBLE_VARIABLES parameter to provide the complete
 * group_vars/all.yml content for Ansible configuration.
 *
**/

// ========================================
// CONSTANTS AND CONFIGURATION
// ========================================
class PipelineConfig {
    static final String DEFAULT_HOSTNAME_PREFIX = 'airgap-ansible-jenkins'
    static final String DEFAULT_RKE2_VERSION = 'v1.28.8+rke2r1'
    static final String DEFAULT_RANCHER_VERSION = 'v2.9-head'
    static final String DEFAULT_RANCHER_TEST_REPO = 'https://github.com/rancher/tests.git'
    static final String DEFAULT_QA_INFRA_REPO = 'https://github.com/rancher/qa-infra-automation.git'
    static final String DEFAULT_S3_BUCKET = 'rancher-terraform-state'
    static final String DEFAULT_S3_REGION = 'us-east-1'
    static final String CONTAINER_NAME_PREFIX = 'rancher-ansible-airgap'
    static final String SHARED_VOLUME_PREFIX = 'validation-volume'
    static final String DOCKER_BUILD_CONTEXT = '.'
    static final String DOCKERFILE_PATH = 'tests/validation/Dockerfile.tofu.e2e'
    static final int TERRAFORM_TIMEOUT_MINUTES = 60
    static final int ANSIBLE_TIMEOUT_MINUTES = 90
    static final int VALIDATION_TIMEOUT_MINUTES = 30
    static final String LOG_PREFIX_INFO = '[INFO]'
    static final String LOG_PREFIX_ERROR = '[ERROR]'
    static final String LOG_PREFIX_WARNING = '[WARNING]'
}

// ========================================
// CONSOLIDATED ENVIRONMENT CONFIGURATION
// ========================================
def configureEnvironmentComplete() {
    logInfo('Configuring complete environment setup')
    
    // Step 1: Read and validate Ansible variables
    readAndValidateAnsibleVariables()
    
    // Step 2: Setup dynamic environment variables
    env.RKE2_VERSION = params.RKE2_VERSION
    env.RANCHER_VERSION = params.RANCHER_VERSION
    env.TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
    env.RANCHER_HOSTNAME = "${params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX}.qa.rancher.space"
    
    logInfo("RKE2 Version: ${env.RKE2_VERSION}")
    logInfo("Rancher Version: ${env.RANCHER_VERSION}")
    logInfo("Rancher Hostname: ${env.RANCHER_HOSTNAME}")
    
    // Step 3: Configure credentials and generate files
    // NOTE: SSH keys must be created AFTER deleteDir() is called
    // This function is called AFTER workspace cleanup
    withCredentials(getCredentialsList()) {
        setupSSHKeysSecure()
        generateEnvironmentFileComplete()
        validateSensitiveDataHandling()
    }
    
    logInfo('Environment configuration completed successfully')
}

/**
 * Read and validate Ansible variables from parameter
 */
def readAndValidateAnsibleVariables() {
    if (!params.ANSIBLE_VARIABLES || !params.ANSIBLE_VARIABLES.trim()) {
        error('ANSIBLE_VARIABLES parameter is required but was not provided')
    }
    
    def ansibleVarsContent = params.ANSIBLE_VARIABLES.trim()
    logInfo("Ansible variables loaded: ${ansibleVarsContent.length()} bytes")
    
    env.ANSIBLE_VARIABLES = ansibleVarsContent
}

/**
 * Setup SSH keys with secure handling
 */
def setupSSHKeysSecure() {
    if (!env.AWS_SSH_PEM_KEY || !env.AWS_SSH_KEY_NAME) {
        logWarning('SSH key configuration skipped - credentials not available')
        return
    }
    
    logInfo('Setting up SSH keys')
    
    try {
        // Create SSH directory in workspace
        dir('./tests/.ssh') {
            sh 'mkdir -p . && chmod 700 .'
            
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
            sh "chown \$(whoami):\$(whoami) ${env.AWS_SSH_KEY_NAME} 2>/dev/null || true"
            
            // Validate key format
            def keyContent = sh(script: "head -1 ${env.AWS_SSH_KEY_NAME}", returnStdout: true).trim()
            if (!keyContent.startsWith('-----BEGIN')) {
                logWarning('SSH key format validation warning - unexpected format')
            }
            
            // Also generate public key for SSH operations
            sh """
                if [ -f "${env.AWS_SSH_KEY_NAME}" ]; then
                    ssh-keygen -f "${env.AWS_SSH_KEY_NAME}" -y > "${env.AWS_SSH_KEY_NAME}.pub" || echo "Failed to generate public key"
                    chmod 644 "${env.AWS_SSH_KEY_NAME}.pub" 2>/dev/null || true
                fi
            """
        }
        
        env.SSH_KEY_PATH = "./tests/.ssh/${env.AWS_SSH_KEY_NAME}"
        logInfo('SSH keys configured successfully')
        
    } catch (Exception e) {
        logError("SSH key setup failed: ${e.message}")
        cleanupSSHKeys()
        throw e
    }
}

/**
 * Generate complete environment file for container execution
 */
def generateEnvironmentFileComplete() {
    logInfo('Generating environment file')
    
    // Parse Terraform config for AWS variables
    def awsVars = extractAWSVariablesFromTerraformConfig()
    
    def envLines = [
        '# Environment Configuration - Credentials passed via withCredentials',
        "TF_WORKSPACE=${env.TF_WORKSPACE}",
        "BUILD_NUMBER=${env.BUILD_NUMBER}",
        "JOB_NAME=${env.JOB_NAME}",
        "TERRAFORM_TIMEOUT=${env.TERRAFORM_TIMEOUT}",
        "ANSIBLE_TIMEOUT=${env.ANSIBLE_TIMEOUT}",
        "QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}",
        "TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}",
        "ANSIBLE_VARS_FILENAME=${env.ANSIBLE_VARS_FILENAME}",
        "RKE2_VERSION=${env.RKE2_VERSION}",
        "RANCHER_VERSION=${env.RANCHER_VERSION}",
        "HOSTNAME_PREFIX=${env.HOSTNAME_PREFIX}",
        "RANCHER_HOSTNAME=${env.RANCHER_HOSTNAME}",
        "PRIVATE_REGISTRY_URL=${env.PRIVATE_REGISTRY_URL}",
        "PRIVATE_REGISTRY_USERNAME=${env.PRIVATE_REGISTRY_USERNAME}",
        '',
        '# Ansible Configuration',
        "ANSIBLE_VARIABLES=${env.ANSIBLE_VARIABLES}",
        '',
        '# AWS Configuration',
        "AWS_REGION=${env.AWS_REGION}",
        "AWS_AMI=${awsVars['AWS_AMI'] ?: ''}",
        "AWS_HOSTNAME_PREFIX=${awsVars['AWS_HOSTNAME_PREFIX'] ?: env.HOSTNAME_PREFIX}",
        "AWS_SSH_USER=${awsVars['AWS_SSH_USER'] ?: 'ec2-user'}",
        "INSTANCE_TYPE=${awsVars['INSTANCE_TYPE'] ?: 't3.large'}",
        '',
        '# S3 Backend',
        "S3_BUCKET_NAME=${env.S3_BUCKET_NAME}",
        "S3_REGION=${env.S3_REGION}",
        "S3_KEY_PREFIX=${env.S3_KEY_PREFIX}",
        '',
        "AWS_SSH_KEY_NAME=${env.AWS_SSH_KEY_NAME ?: ''}",
        '# Sensitive credentials passed via withCredentials block'
    ]
    
    writeFile file: env.ENV_FILE, text: envLines.join('\n')
    logInfo("Environment file created: ${env.ENV_FILE}")
}

/**
 * Extract AWS variables from Terraform config parameter
 */
def extractAWSVariablesFromTerraformConfig() {
    def awsVars = [:]
    
    if (!env.TERRAFORM_CONFIG) {
        return awsVars
    }
    
    def config = env.TERRAFORM_CONFIG
    def pattern = ~/(\w+)\s*=\s*"([^"]*)"/
    
    config.eachMatch(pattern) { match ->
        def varName = match[1]
        def varValue = match[2]
        if (varName.startsWith('AWS_') || varName == 'INSTANCE_TYPE') {
            awsVars[varName] = varValue
        }
    }
    
    return awsVars
}

// ========================================
// PIPELINE DEFINITION
// ========================================

// Lazily-loadable Docker helper accessor (must be defined before pipeline execution)
dockerHelperInstance = null

def dockerHelper() {
    if (dockerHelperInstance == null) {
        // Try a few likely locations for the helper file. Jenkins may perform
        // additional checkouts (into subdirs like 'tests') which moves file
        // paths around during the run; be tolerant and try multiple candidates.
        def candidates = [
            'validation/pipeline/scripts/airgap/docker_helper.groovy',
            'tests/validation/pipeline/scripts/airgap/docker_helper.groovy'
        ]

        def helperPath = null
        for (p in candidates) {
            try {
                if (fileExists(p)) {
                    helperPath = p
                    break
                }
            } catch (ignored) {
                // fileExists may not be available outside node context; ignore and continue
            }
        }

        if (helperPath == null) {
            echo "[ERROR] docker_helper.groovy not found in workspace at any of: ${candidates}"
            // Provide workspace diagnostics to help triage
            try {
                echo '[DEBUG] Listing validation/pipeline/scripts/airgap:'
                sh 'ls -la validation/pipeline/scripts/airgap || true'
                echo '[DEBUG] Listing tests/validation/pipeline/scripts/aigap:'
                sh 'ls -la tests/validation/pipeline/scripts/airgap || true'
            } catch (err) {
                echo "[DEBUG] workspace listing failed: ${err.message}"
            }
            error('docker_helper.groovy not present in workspace; ensure the repo checkout includes validation/pipeline/scripts or adjust pipeline checkouts')
        }

        def helperScript = load(helperPath)
        dockerHelperInstance = helperScript.init(this)
    }
    return dockerHelperInstance
}

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 3, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
        retry(1)
    }

    // Environment-specific parameters
    parameters {
        string(
            name: 'RKE2_VERSION',
            defaultValue: PipelineConfig.DEFAULT_RKE2_VERSION,
            description: 'RKE2 version to deploy (e.g., v1.28.8+rke2r1, v1.29.5+rke2r1, v1.30.2+rke2r1)'
        )
        string(
            name: 'RANCHER_VERSION',
            defaultValue: PipelineConfig.DEFAULT_RANCHER_VERSION,
            description: 'Rancher version to deploy (e.g., head, v2.10-head, v2.11.0, v2.9-head)'
        )
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: PipelineConfig.DEFAULT_RANCHER_TEST_REPO,
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: PipelineConfig.DEFAULT_QA_INFRA_REPO,
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'PRIVATE_REGISTRY_URL',
            defaultValue: '',
            description: 'Private registry URL for airgap deployment'
        )
        string(
            name: 'PRIVATE_REGISTRY_USERNAME',
            defaultValue: 'default-user',
            description: 'Private registry username for airgap deployment'
        )
        password(
            name: 'PRIVATE_REGISTRY_PASSWORD',
            defaultValue: '',
            description: 'Private registry password for airgap deployment'
        )
        string(
            name: 'S3_BUCKET_NAME',
            defaultValue: PipelineConfig.DEFAULT_S3_BUCKET,
            description: 'S3 bucket name where Terraform state is stored'
        )
        string(
            name: 'S3_KEY_PREFIX',
            defaultValue: 'jenkins-airgap-rke2/',
            description: 'S3 key prefix for the Terraform state files'
        )
        string(
            name: 'S3_REGION',
            defaultValue: PipelineConfig.DEFAULT_S3_REGION,
            description: 'AWS region where the S3 bucket is located'
        )
        string(
            name: 'HOSTNAME_PREFIX',
            defaultValue: PipelineConfig.DEFAULT_HOSTNAME_PREFIX,
            description: 'Hostname prefix for *.qa.rancher.space and other AWS resources'
        )
        booleanParam(
            name: 'DESTROY_ON_FAILURE',
            defaultValue: true,
            description: 'Destroy infrastructure when Ansible playbooks fail (automatic cleanup)'
        )
        text(
            name: 'TERRAFORM_CONFIG',
            defaultValue: '',
            description: 'Terraform variables configuration for OpenTofu deployment'
        )
        text(
            name: 'ANSIBLE_VARIABLES',
            description: 'These config values are for the rancher instance use for the recurring runs.'
        )
        booleanParam(
            name: 'SKIP_YAML_VALIDATION',
            defaultValue: false,
            description: 'Skip strict YAML validation for templated group_vars payloads'
        )
        booleanParam(
            name: 'DEBUG',
            defaultValue: false,
            description: 'Enable debug output and verbose logging throughout the pipeline'
        )
    }

    // Global environment variables
    environment {
        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: PipelineConfig.DEFAULT_RANCHER_TEST_REPO}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: PipelineConfig.DEFAULT_QA_INFRA_REPO}"

        // Private registry configurations
        PRIVATE_REGISTRY_URL = "${params.PRIVATE_REGISTRY_URL ?: ''}"
        PRIVATE_REGISTRY_USERNAME = "${params.PRIVATE_REGISTRY_USERNAME ?: 'default-user'}"
        PRIVATE_REGISTRY_PASSWORD = "${params.PRIVATE_REGISTRY_PASSWORD ?: ''}"

        // Path configurations
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'

        // Cleanup configurations
        DESTROY_ON_FAILURE = "${params.DESTROY_ON_FAILURE}"

        // Skip strict YAML Validation
        SKIP_YAML_VALIDATION = "${params.SKIP_YAML_VALIDATION}"

        // Debug mode
        DEBUG = "${params.DEBUG}"

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${PipelineConfig.CONTAINER_NAME_PREFIX}-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"
        IMAGE_NAME = "rancher-ansible-airgap-setup-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"
        VALIDATION_VOLUME = "${PipelineConfig.SHARED_VOLUME_PREFIX}-${env.JOB_SHORT_NAME}${env.BUILD_NUMBER}"

        // Configuration files
        ANSIBLE_VARS_FILENAME = 'vars.yaml'
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        TERRAFORM_BACKEND_CONFIG_FILENAME = 'backend.tf'
        ENV_FILE = '.env'

        // Terraform workspace
        TF_WORKSPACE = "jenkins_airgap_ansible_workspace_${env.BUILD_NUMBER}"

        // Timeouts (in minutes)
        TERRAFORM_TIMEOUT = "${PipelineConfig.TERRAFORM_TIMEOUT_MINUTES}"
        ANSIBLE_TIMEOUT = "${PipelineConfig.ANSIBLE_TIMEOUT_MINUTES}"
        VALIDATION_TIMEOUT = "${PipelineConfig.VALIDATION_TIMEOUT_MINUTES}"

        // Backend configuration (S3 backend parameters)
        S3_BUCKET_NAME = "${params.S3_BUCKET_NAME ?: PipelineConfig.DEFAULT_S3_BUCKET}"
        S3_REGION = "${params.S3_REGION ?: PipelineConfig.DEFAULT_S3_REGION}"
        AWS_REGION = "${params.S3_REGION ?: PipelineConfig.DEFAULT_S3_REGION}"
        S3_KEY_PREFIX = "${params.S3_KEY_PREFIX ?: 'jenkins-airgap-rke2'}"

        // Hostname prefix
        HOSTNAME_PREFIX = "${params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX}"
        RANCHER_HOSTNAME = "${(params.HOSTNAME_PREFIX ?: PipelineConfig.DEFAULT_HOSTNAME_PREFIX)}.qa.rancher.space"

        // Configuration content from parameters
        TERRAFORM_CONFIG = "${params.TERRAFORM_CONFIG ?: ''}"
        ANSIBLE_VARIABLES = "${params.ANSIBLE_VARIABLES ?: ''}"
    }

    stages {
        stage('Initialize Pipeline') {
            steps {
                script {
                    // Clean workspace
                    deleteDir()

                    // Complete environment configuration
                    configureEnvironmentComplete()

                    logInfo("Build container: ${env.BUILD_CONTAINER_NAME}")
                    logInfo("Docker image: ${env.IMAGE_NAME}")
                    logInfo("Volume: ${env.VALIDATION_VOLUME}")
                }
            }
        }

        stage('Checkout Repositories') {
            steps {
                script {
                    logInfo('Checking out source repositories')

                    // Checkout Rancher Tests Repository
                    dir('./tests') {
                        logInfo("Cloning rancher tests repository from ${env.RANCHER_TEST_REPO_URL}")
                        checkout([
                            $class: 'GitSCM',
                            branches: [[name: "*/${params.RANCHER_TEST_REPO_BRANCH}"]],
                            extensions: [
                                [$class: 'CleanCheckout'],
                                [$class: 'CloneOption', depth: 1, shallow: true]
                            ],
                            userRemoteConfigs: [[
                                url: env.RANCHER_TEST_REPO_URL,
                            ]]
                        ])
                    }

                    // Checkout QA Infrastructure Repository
                    dir('./qa-infra-automation') {
                        logInfo("Cloning qa-infra-automation repository from ${env.QA_INFRA_REPO}")
                        logInfo("Using branch: ${params.QA_INFRA_REPO_BRANCH}")
                        checkout([
                            $class: 'GitSCM',
                            branches: [[name: "*/${params.QA_INFRA_REPO_BRANCH}"]],
                            extensions: [
                                [$class: 'CleanCheckout'],
                                [$class: 'CloneOption', depth: 1, shallow: true]
                            ],
                            userRemoteConfigs: [[
                                url: env.QA_INFRA_REPO,
                            ]]
                        ])
                        // Verify which branch was actually checked out
                        def actualBranch = sh(script: 'git rev-parse --abbrev-ref HEAD', returnStdout: true).trim()
                        def latestCommit = sh(script: 'git log -1 --oneline', returnStdout: true).trim()
                        logInfo("Checked out branch: ${actualBranch}")
                        logInfo("Latest commit: ${latestCommit}")
                    }

                    logInfo('Repository checkout completed successfully')
                }
            }
        }

        stage('Configure Environment') {
            steps {
                script {
                    logInfo('Configuring deployment environment')

                    // Configure credentials and environment files
                    withCredentials(getCredentialsList()) {

                        // Validate sensitive data handling and security measures
                        validateSensitiveDataHandling()
                    }
                }
            }
        }

        stage('Prepare Infrastructure') {
            steps {
                script {
                    logInfo('Preparing infrastructure components')

                    // Configure credentials for Docker operations
                    withCredentials(getCredentialsList()) {
                        // Build Docker image with proper tagging
                        buildDockerImage()

                        // Create shared volume
                        createSharedVolume()

                        // Copy SSH keys to container volume AFTER volume is created
                        ensureSSHKeysInContainer()

                        // Validate parameters and environment
                        validateParameters()
                    }
                }
            }
        }

        stage('Deploy Infrastructure') {
            steps {
                script {
                    logInfo('Deploying infrastructure')

                    // Validate required variables inline
                    validateRequiredVariables([
                            'QA_INFRA_WORK_PATH', 'TF_WORKSPACE',
                            'TERRAFORM_VARS_FILENAME', 'TERRAFORM_BACKEND_CONFIG_FILENAME'
                    ])

                    // Generate configuration files
                    generateTofuConfiguration()

                    def timeoutMinutes = env.TERRAFORM_TIMEOUT ?
                            Integer.parseInt(env.TERRAFORM_TIMEOUT) : PipelineConfig.TERRAFORM_TIMEOUT_MINUTES

                    timeout(time: timeoutMinutes, unit: 'MINUTES') {
                        try {
                            // Direct script execution - no wrapper function
                            def infraScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_infrastructure_deploy.sh
'''

                            def infraEnvVars = [
                                    'RKE2_VERSION': env.RKE2_VERSION,
                                    'RANCHER_VERSION': env.RANCHER_VERSION,
                                    'HOSTNAME_PREFIX': env.HOSTNAME_PREFIX,
                                    'RANCHER_HOSTNAME': env.RANCHER_HOSTNAME,
                                    'PRIVATE_REGISTRY_URL': env.PRIVATE_REGISTRY_URL,
                                    'PRIVATE_REGISTRY_USERNAME': env.PRIVATE_REGISTRY_USERNAME,
                                    'UPLOAD_CONFIG_TO_S3': 'true',
                                    'S3_BUCKET_NAME': env.S3_BUCKET_NAME,
                                    'S3_REGION': env.S3_REGION,
                                    'S3_KEY_PREFIX': env.S3_KEY_PREFIX,
                                    'AWS_REGION': env.AWS_REGION,
                                    'AWS_ACCESS_KEY_ID': env.AWS_ACCESS_KEY_ID,
                                    'AWS_SECRET_ACCESS_KEY': env.AWS_SECRET_ACCESS_KEY,
                                    'AWS_SSH_PEM_KEY': env.AWS_SSH_PEM_KEY,
                                    'AWS_SSH_KEY_NAME': env.AWS_SSH_KEY_NAME
                            ]

                            dockerHelper().executeScriptInContainer(infraScript, infraEnvVars)
                            extractArtifactsFromDockerVolume()
                            logInfo('Infrastructure deployed successfully')

                        } catch (InterruptedException ignored) {
                            handleFailureCleanup('timeout')
                        } catch (Exception e) {
                            logError("Infrastructure deployment failed: ${e.message}")
                            handleFailureCleanup('deployment')
                        }
                    }
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('deployment') }
                }
            }
        }

        stage('Prepare Ansible Environment') {
            steps {
                script {
                    logInfo('Preparing Ansible environment with consolidated script')

                    validateRequiredVariables(['QA_INFRA_WORK_PATH', 'ANSIBLE_VARS_FILENAME'])

                    def timeoutMinutes = env.ANSIBLE_TIMEOUT ?
                            Integer.parseInt(env.ANSIBLE_TIMEOUT) : PipelineConfig.ANSIBLE_TIMEOUT_MINUTES

                    timeout(time: timeoutMinutes, unit: 'MINUTES') {
                        // Direct script execution - no wrapper
                        def ansiblePrepScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/ansible_prepare_environment.sh
'''

                        def ansiblePrepEnvVars = [
                                'ANSIBLE_VARIABLES': env.ANSIBLE_VARIABLES,
                                'RKE2_VERSION': env.RKE2_VERSION,
                                'RANCHER_VERSION': env.RANCHER_VERSION,
                                'HOSTNAME_PREFIX': env.HOSTNAME_PREFIX,
                                'RANCHER_HOSTNAME': env.RANCHER_HOSTNAME,
                                'PRIVATE_REGISTRY_URL': env.PRIVATE_REGISTRY_URL,
                                'PRIVATE_REGISTRY_USERNAME': env.PRIVATE_REGISTRY_USERNAME,
                                'PRIVATE_REGISTRY_PASSWORD': env.PRIVATE_REGISTRY_PASSWORD,
                                'SKIP_YAML_VALIDATION': env.SKIP_YAML_VALIDATION,
                                'AWS_SSH_PEM_KEY': env.AWS_SSH_PEM_KEY,
                                'AWS_SSH_KEY_NAME': env.AWS_SSH_KEY_NAME
                        ]

                        dockerHelper().executeScriptInContainer(ansiblePrepScript, ansiblePrepEnvVars)
                    }
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('ansible_prep') }
                }
            }
        }

        stage('Deploy RKE2 with Ansible') {
            steps {
                script {
                    logInfo('Deploying RKE2 cluster with consolidated script')

                    validateRequiredVariables(['QA_INFRA_WORK_PATH', 'ANSIBLE_VARS_FILENAME'])

                    def timeoutMinutes = env.ANSIBLE_TIMEOUT ?
                            Integer.parseInt(env.ANSIBLE_TIMEOUT) : PipelineConfig.ANSIBLE_TIMEOUT_MINUTES

                    timeout(time: timeoutMinutes, unit: 'MINUTES') {
                        try {
                            // Direct script execution - no wrapper
                            def rke2Script = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/ansible_deploy_rke2.sh
'''

                            def rke2EnvVars = [
                                    'RKE2_VERSION': env.RKE2_VERSION,
                                    'SKIP_VALIDATION': 'false',
                                    'AWS_SSH_PEM_KEY': env.AWS_SSH_PEM_KEY,
                                    'AWS_SSH_KEY_NAME': env.AWS_SSH_KEY_NAME
                            ]

                            dockerHelper().executeScriptInContainer(rke2Script, rke2EnvVars)
                            logInfo('RKE2 deployment completed successfully')

                        } catch (Exception e) {
                            logError("RKE2 deployment failed: ${e.message}")
                            handleFailureCleanup('rke2')
                            throw e
                        }
                    }
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('rke2') }
                }
            }
        }

        stage('Deploy Rancher with Ansible') {
            steps {
                script {
                    logInfo('Deploying Rancher with consolidated script')

                    validateRequiredVariables(['QA_INFRA_WORK_PATH', 'ANSIBLE_VARS_FILENAME'])

                    def timeoutMinutes = env.ANSIBLE_TIMEOUT ?
                            Integer.parseInt(env.ANSIBLE_TIMEOUT) : PipelineConfig.ANSIBLE_TIMEOUT_MINUTES

                    timeout(time: timeoutMinutes, unit: 'MINUTES') {
                        try {
                            // Direct script execution - no wrapper
                            def rancherScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/ansible_deploy_rancher.sh
'''

                            def rancherEnvVars = [
                                    'RANCHER_VERSION': env.RANCHER_VERSION,
                                    'HOSTNAME_PREFIX': env.HOSTNAME_PREFIX,
                                    'RANCHER_HOSTNAME': env.RANCHER_HOSTNAME,
                                    'SKIP_VERIFICATION': 'false'
                            ]

                            dockerHelper().executeScriptInContainer(rancherScript, rancherEnvVars)
                            logInfo('Rancher deployment completed successfully')

                        } catch (Exception e) {
                            logError("Rancher deployment failed: ${e.message}")
                            handleFailureCleanup('rancher')
                            throw e
                        }
                    }
                }
            }
            post {
                failure {
                    script { handleFailureCleanup('rancher') }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-build cleanup')

                // Archive important artifacts (excluding sensitive tfstate and tfvars files)
                archiveBuildArtifacts([
                    'kubeconfig.yaml',
                    'infrastructure-outputs.json',
                    'ansible-inventory.yml',
                    'ansible-logs.txt',
                    'deployment-summary.json'
                ])

                // Always cleanup containers and volumes
                try {
                    node {
                        cleanupContainersAndVolumes()
                    }
                } catch (Exception e) {
                    logError("Node context not available for cleanup: ${e.message}")
                    try {
                        cleanupContainersAndVolumes()
                    } catch (Exception cleanupException) {
                        logError("Cleanup failed: ${cleanupException.message}")
                    }
                }
            }
        }

        success {
            script {
                logInfo('Pipeline completed successfully')
            }
        }

        failure {
            script {
                logError('Pipeline failed')

                // Use consolidated cleanup script for failure
                if (env.DESTROY_ON_FAILURE.toBoolean()) {
                    logInfo('DESTROY_ON_FAILURE is true - using consolidated cleanup script for pipeline failure')
                    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup for deployment failure
perform_cleanup "deployment_failure" "${TF_WORKSPACE}" "true"
'''
                    def cleanupEnvVars = [
                        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
                        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
                        'TF_WORKSPACE': env.TF_WORKSPACE,
                        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
                        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
                    ]
                    dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
                }
            }
        }

        aborted {
            script {
                logWarning('Pipeline was aborted')

                // Use consolidated cleanup script for timeout/abort
                if (env.DESTROY_ON_FAILURE.toBoolean()) {
                    logInfo('DESTROY_ON_FAILURE is true - using consolidated cleanup script for pipeline timeout')
                    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup for timeout
perform_cleanup "timeout" "${TF_WORKSPACE}" "true"
'''
                    def cleanupEnvVars = [
                        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
                        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
                        'TF_WORKSPACE': env.TF_WORKSPACE,
                        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
                        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
                    ]
                    dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
                }
            }
        }

        unstable {
            script {
                logWarning('Pipeline completed with warnings')
            }
        }
    }
}

// ========================================
// LOGGING UTILITY FUNCTIONS
// ========================================


def logInfo(msg) {
    echo "${PipelineConfig.LOG_PREFIX_INFO} ${getTimestamp()} ${msg}"
}

def logError(msg) {
    echo "${PipelineConfig.LOG_PREFIX_ERROR} ${getTimestamp()} ${msg}"
}

def logWarning(msg) {
    echo "${PipelineConfig.LOG_PREFIX_WARNING} ${getTimestamp()} ${msg}"
}


static def getTimestamp() {
    return new Date().format('yyyy-MM-dd HH:mm:ss')
}

// ========================================
// PARAMETER VALIDATION FUNCTIONS
// ========================================

def validateParameters() {
    logInfo('Validating pipeline parameters using external script')

    def validationScript = '''
#!/bin/bash
set -e
# Source the external validation helper script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
# Run parameter validation
validate_pipeline_parameters
'''

    def validationEnvVars = [
            'RKE2_VERSION': params.RKE2_VERSION,
            'RANCHER_VERSION': params.RANCHER_VERSION,
            'RANCHER_TEST_REPO_URL': params.RANCHER_TEST_REPO_URL,
            'QA_INFRA_REPO_URL': params.QA_INFRA_REPO_URL,
    ]

    try {
        dockerHelper().executeScriptInContainer(validationScript, validationEnvVars)
        logInfo('All parameters validated successfully')
    } catch (Exception e) {
        def errorMsg = "Parameter validation failed: ${e.message}"
        logError(errorMsg)
        error(errorMsg)
    }
}

def validateRequiredVariables(List<String> requiredVars) {
    logInfo('Validating required environment variables')

    def validationScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
validate_required_variables ''' + requiredVars.join(' ')

    def envVars = [:]
    requiredVars.each { varName ->
        envVars[varName] = env."${varName}" ?: ''
    }

    try {
        dockerHelper().executeScriptInContainer(validationScript, envVars)
        logInfo('All required variables validated successfully')
    } catch (Exception e) {
        def errorMsg = "Required variables validation failed: ${e.message}"
        logError(errorMsg)
        error(errorMsg)
    }
}


def validateSensitiveDataHandling() {
    logInfo('Validating sensitive data handling using external script')

    def validationScript = '''
#!/bin/bash
set -e
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/validation_helpers.sh
validate_sensitive_data_handling "${ENV_FILE}"
'''

    def validationEnvVars = [
            'ENV_FILE': env.ENV_FILE,
            'SSH_KEY_PATH': env.SSH_KEY_PATH ?: ''
    ]

    try {
        dockerHelper().executeScriptInContainer(validationScript, validationEnvVars)
        logInfo('Sensitive data handling validated successfully')
    } catch (Exception e) {
        // Log warning but don't fail the build
        logWarning("Sensitive data validation issues detected: ${e.message}")
        logWarning('Review security measures and address any concerns')
    }
}

// ========================================
// ENVIRONMENT SETUP FUNCTIONS
// ========================================

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'AWS_SSH_PEM_KEY', variable: 'AWS_SSH_PEM_KEY'),
        string(credentialsId: 'AWS_SSH_KEY_NAME', variable: 'AWS_SSH_KEY_NAME'),
    ]
}

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

// ========================================
// CONFIGURATION GENERATION FUNCTIONS
// ========================================

def cleanupSSHKeys() {
    logInfo('Cleaning up SSH keys securely')

    try {
        // Get credentials for key name
        withCredentials([
            string(credentialsId: 'AWS_SSH_KEY_NAME', variable: 'AWS_SSH_KEY_NAME')
        ]) {
            def awsSshKeyName = env.AWS_SSH_KEY_NAME

            if (awsSshKeyName) {
                def keyPath = "./tests/.ssh/${awsSshKeyName}"

                if (fileExists(keyPath)) {
                    // Securely shred the key file if shred is available
                    try {
                        sh "shred -vfz -n 3 ${keyPath} 2>/dev/null || rm -f ${keyPath}"
                        logInfo("SSH key securely shredded: ${keyPath}")
                    } catch (Exception ignored) {
                        // Fallback to secure delete
                        sh "rm -f ${keyPath}"
                        logWarning("SSH key deleted (shred unavailable): ${keyPath}")
                    }
                }

                // Clean up any temporary SSH files
                sh 'rm -f ./tests/.ssh/known_hosts ./tests/.ssh/config 2>/dev/null || true'

                // Ensure SSH directory is secure
                if (fileExists('./tests/.ssh')) {
                    sh 'chmod 700 ./tests/.ssh 2>/dev/null || true'
                }
            }
        }
    } catch (Exception e) {
        logWarning("SSH key cleanup encountered issues: ${e.message}")
    // Continue with cleanup even if some steps fail
    }

    logInfo('SSH key cleanup completed')
}


// ========================================
// DOCKER MANAGEMENT FUNCTIONS
// ========================================

def buildDockerImage() {
    logInfo("Building Docker image: ${env.IMAGE_NAME}")

    dir(PipelineConfig.DOCKER_BUILD_CONTEXT) {
        // Run configure script silently
        sh './tests/validation/configure.sh > /dev/null 2>&1'

        // Build Docker image with minimal output
        sh """
            docker build . \\
                -f ${PipelineConfig.DOCKERFILE_PATH} \\
                -t ${env.IMAGE_NAME} \\
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \\
                --build-arg VCS_REF=\$(git rev-parse --short HEAD) \\
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \\
                --label "pipeline.job.name=${env.JOB_NAME}" \\
                --quiet
        """
    }

    logInfo('Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def ensureSSHKeysInContainer() {
    logInfo('Ensuring SSH keys are available in container')

    def sshKeyName = env.AWS_SSH_KEY_NAME
    if (!sshKeyName) {
        logWarning('AWS_SSH_KEY_NAME not set, cannot copy SSH keys')
        return
    }

    def sshDir = "./tests/.ssh"
    def keyPath = "${sshDir}/${sshKeyName}"

    // Debug: Check what exists
    sh """
        echo "Checking for SSH keys..."
        echo "SSH Directory: ${sshDir}"
        echo "Key Name: ${sshKeyName}"
        echo "Expected Path: ${keyPath}"
        ls -la ${sshDir}/ || echo "SSH directory not found"
        test -f ${keyPath} && echo "SSH key file EXISTS" || echo "SSH key file NOT FOUND"
    """

    if (!fileExists(keyPath)) {
        logWarning("SSH key not found at: ${keyPath}")
        logWarning('Keys may not have been created properly')

        // Try to recreate them
        withCredentials(getCredentialsList()) {
            try {
                setupSSHKeysSecure()
                logInfo('SSH keys recreated')
            } catch (Exception e) {
                logError("Failed to recreate SSH keys: ${e.message}")
                return
            }
        }
    }

    try {
        // Copy SSH keys into container volume
        sh """
            echo "Copying SSH keys to container volume..."
            docker run --rm \\
                -v ${env.VALIDATION_VOLUME}:/target \\
                -v \$(pwd)/${sshDir}:/source:ro \\
                alpine:latest \\
                sh -c '
                    set -x
                    mkdir -p /target/.ssh
                    chmod 700 /target/.ssh
                    cp -v /source/* /target/.ssh/ || { echo "Failed to copy keys"; exit 1; }
                    chmod 600 /target/.ssh/* || true
                    echo "SSH keys copied successfully"
                    ls -la /target/.ssh/
                '
        """
        logInfo('SSH keys successfully copied to container volume')
    } catch (Exception e) {
        logError("Failed to copy SSH keys to container: ${e.message}")
        throw e
    }
}

def cleanupContainersAndVolumes() {
    logInfo('Cleaning up Docker containers and volumes')

    try {
        if (env.NODE_NAME) {
            sh """
                # Stop and remove any containers with our naming pattern
                if docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | grep -q .; then
                    docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
                    docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true
                    echo "Stopped and removed containers for ${env.BUILD_CONTAINER_NAME}"
                else
                    echo "No containers found for ${env.BUILD_CONTAINER_NAME}"
                fi

                # Remove the Docker image if it exists
                if docker images -q ${env.IMAGE_NAME} | grep -q .; then
                    docker rmi -f ${env.IMAGE_NAME} || true
                    echo "Removed Docker image ${env.IMAGE_NAME}"
                else
                    echo "Docker image ${env.IMAGE_NAME} not found or already removed"
                fi

                # Remove the shared volume if it exists
                if docker volume ls -q | grep -q "^${env.VALIDATION_VOLUME}\$"; then
                    docker volume rm -f ${env.VALIDATION_VOLUME} || true
                    echo "Removed Docker volume ${env.VALIDATION_VOLUME}"
                else
                    echo "Docker volume ${env.VALIDATION_VOLUME} not found or already removed"
                fi

                # Clean up any dangling images and volumes
                docker system prune -f || true
                echo "Docker cleanup completed"
            """
        } else {
            logWarning('No node context available for Docker cleanup')
        }
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }

    // Clean up SSH keys securely
    try {
        cleanupSSHKeys()
    } catch (Exception sshCleanupException) {
        logWarning("SSH key cleanup encountered issues: ${sshCleanupException.message}")
    }

    // Clean up environment files containing sensitive data
    try {
        if (fileExists(env.ENV_FILE)) {
            sh "shred -vfz -n 3 ${env.ENV_FILE} 2>/dev/null || rm -f ${env.ENV_FILE}"
            logInfo('Environment file securely shredded')
        }
    } catch (Exception envCleanupException) {
        logWarning("Environment file cleanup encountered issues: ${envCleanupException.message}")
    }
}

// ========================================
// UNIFIED ARTIFACT MANAGEMENT
// ========================================

/**
 * Centralized artifact configuration map
 * Defines all artifact types and their associated files
 */
@NonCPS
static def getArtifactDefinitions() {
    return [
        'infrastructure': [
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            '*.tfvars'
        ],
        'ansible_prep': [
            'group_vars.tar.gz',
            'group_vars/all.yml',
            'ansible-preparation-report.txt'
        ],
        'rke2_deployment': [
            'kubeconfig.yaml',
            'rke2_deployment_report.txt',
            'rke2_deployment.log',
            'kubectl-setup-logs.txt'
        ],
        'rancher_deployment': [
            'rancher-deployment-logs.txt',
            'rancher-validation-logs.txt',
            'deployment-summary.json'
        ],
        'failure_common': [
            '*.log',
            'error-*.txt',
            'ansible-debug-info.txt'
        ],
        'failure_infrastructure': [
            'tfplan-backup',
            'infrastructure-outputs.json'
        ],
        'failure_ansible': [
            'ansible-inventory.yml',
            'ansible-error-logs.txt',
            'ssh-setup-error-logs.txt'
        ],
        'failure_rke2': [
            'rke2-deployment-error-logs.txt',
            'kubectl-setup-error-logs.txt'
        ],
        'failure_rancher': [
            'rancher-deployment-error-logs.txt',
            'rancher-validation-logs.txt',
            'rancher-debug-info.txt'
        ],
        'success_complete': [
            'kubeconfig.yaml',
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            'deployment-summary.json'
        ]
    ]
}

/**
 * Archive artifacts based on type
 */
def archiveArtifactsByType(String artifactType, List additionalFiles = []) {
    def artifactDefs = getArtifactDefinitions()
    
    if (!artifactDefs.containsKey(artifactType)) {
        logWarning("Unknown artifact type: ${artifactType}, using failure_common")
        artifactType = 'failure_common'
    }
    
    def artifactList = artifactDefs[artifactType] + additionalFiles
    
    logInfo("Archiving ${artifactType} artifacts")
    try {
        archiveArtifacts(
            artifacts: artifactList.join(','),
            allowEmptyArchive: true,
            fingerprint: true,
            onlyIfSuccessful: false
        )
    } catch (Exception e) {
        logError("Failed to archive ${artifactType} artifacts: ${e.message}")
    }
}

/**
 * Archive failure artifacts - combines common + specific
 */
def archiveFailureArtifactsByType(String failureType) {
    def typeMap = [
        'deployment': 'failure_infrastructure',
        'ansible_prep': 'failure_ansible',
        'rke2': 'failure_rke2',
        'rancher': 'failure_rancher',
        'timeout': 'failure_infrastructure',
        'aborted': 'failure_common'
    ]
    
    def specificType = typeMap[failureType] ?: 'failure_common'
    archiveArtifactsByType(specificType)
    
    if (specificType != 'failure_common') {
        archiveArtifactsByType('failure_common')
    }
}

def archiveBuildArtifacts(List artifacts) {
    try {
        archiveArtifacts(
            artifacts: artifacts.join(','),
            allowEmptyArchive: true
        )
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

/**
 * Unified cleanup handler for all failure scenarios
 * Replaces repetitive cleanup code in multiple post blocks
 *
 * @param failureType Type of failure: 'deployment', 'ansible_prep', 'rke2', 'rancher', 'timeout', 'aborted'
 */
def handleFailureCleanup(String failureType) {
    logError("${failureType} failure detected - initiating cleanup")

    try {
        // Extract artifacts before cleanup
        extractArtifactsFromDockerVolume()

        // Archive failure-specific artifacts
        archiveFailureArtifactsByType(failureType)

        // Execute infrastructure cleanup if enabled
        if (env.DESTROY_ON_FAILURE?.toBoolean()) {
            logInfo('DESTROY_ON_FAILURE is true - executing consolidated cleanup')
            executeInfrastructureCleanup(failureType)
        } else {
            logWarning('DESTROY_ON_FAILURE is false - manual cleanup required')
            logWarning("Please clean up resources in workspace: ${env.TF_WORKSPACE}")
        }

        // Always cleanup containers and volumes
        cleanupContainersAndVolumes()

    } catch (Exception cleanupException) {
        logError("Cleanup failed: ${cleanupException.message}")
        // Attempt container cleanup even if infrastructure cleanup fails
        try {
            cleanupContainersAndVolumes()
        } catch (Exception containerException) {
            logError("Container cleanup also failed: ${containerException.message}")
        }
    }
}

/**
 * Execute infrastructure cleanup using consolidated script
 *
 * @param cleanupReason Reason for cleanup: 'deployment_failure', 'timeout', 'aborted'
 */
def executeInfrastructureCleanup(String failureType) {
    def cleanupReason = failureType in ['timeout', 'aborted'] ? failureType : 'deployment_failure'

    def cleanupScript = '''
#!/bin/bash
set -e
# Source the consolidated cleanup script
source /root/go/src/github.com/rancher/tests/validation/pipeline/scripts/airgap/airgap_cleanup.sh
# Execute cleanup
perform_cleanup "${CLEANUP_REASON}" "${TF_WORKSPACE}" "true"
'''

    def cleanupEnvVars = [
        'CLEANUP_REASON': cleanupReason,
        'DESTROY_ON_FAILURE': env.DESTROY_ON_FAILURE,
        'QA_INFRA_WORK_PATH': env.QA_INFRA_WORK_PATH,
        'TF_WORKSPACE': env.TF_WORKSPACE,
        'TERRAFORM_BACKEND_CONFIG_FILENAME': env.TERRAFORM_BACKEND_CONFIG_FILENAME,
        'TERRAFORM_VARS_FILENAME': env.TERRAFORM_VARS_FILENAME
    ]

    try {
        dockerHelper().executeScriptInContainer(cleanupScript, cleanupEnvVars)
        logInfo("Infrastructure cleanup completed for ${failureType}")
    } catch (Exception e) {
        logError("Infrastructure cleanup failed: ${e.message}")
        throw e
    }
}

// ========================================
// ARTIFACT EXTRACTION FUNCTIONS
// ========================================

def extractArtifactsFromDockerVolume() {
    logInfo('Extracting artifacts from Docker shared volume to Jenkins workspace')

    try {
        // Create a temporary container to copy files from the shared volume
        def timestamp = System.currentTimeMillis()
        def extractorContainerName = "${env.BUILD_CONTAINER_NAME}-extractor-${timestamp}"

        sh """
            # Create extractor container to copy files from shared volume
            docker run --rm \\
                -v ${env.VALIDATION_VOLUME}:/source \\
                -v \$(pwd):/dest \\
                --name ${extractorContainerName} \\
                alpine:latest \\
                sh -c '
                    echo "Copying artifacts from shared volume to Jenkins workspace..."

                    # Copy infrastructure outputs if exists
                    if [ -f /source/infrastructure-outputs.json ]; then
                        cp /source/infrastructure-outputs.json /dest/
                        echo "[OK] Copied infrastructure-outputs.json"
                    else
                        echo "[WARN] infrastructure-outputs.json not found in shared volume"
                    fi

                    # Copy ansible inventory if exists (generated by Terraform)
                    if [ -f /source/ansible/rke2/airgap/inventory.yml ]; then
                        cp /source/ansible/rke2/airgap/inventory.yml /dest/ansible-inventory.yml
                        echo "[OK] Copied ansible-inventory.yml from Terraform generated location"
                    elif [ -f /source/ansible-inventory.yml ]; then
                        cp /source/ansible-inventory.yml /dest/
                        echo "[OK] Copied ansible-inventory.yml from legacy location"
                    else
                        echo "[WARN] ansible-inventory.yml not found in shared volume"
                    fi

                    # Copy terraform vars file if exists
                    if [ -f /source/${env.TERRAFORM_VARS_FILENAME} ]; then
                        cp /source/${env.TERRAFORM_VARS_FILENAME} /dest/
                        echo "[OK] Copied ${env.TERRAFORM_VARS_FILENAME}"
                    else
                        echo "[WARN] ${env.TERRAFORM_VARS_FILENAME} not found in shared volume"
                    fi

                    # Copy terraform state files if they exist
                    if [ -f /source/terraform.tfstate ]; then
                        cp /source/terraform.tfstate /dest/
                        echo "[OK] Copied terraform.tfstate"
                    fi

                    if [ -f /source/terraform-state-primary.tfstate ]; then
                        cp /source/terraform-state-primary.tfstate /dest/
                        echo "[OK] Copied terraform-state-primary.tfstate"
                    fi

                    # Copy any backup state files
                    for backup_file in /source/terraform-state-backup-*.tfstate /source/tfstate-backup-*.tfstate; do
                        if [ -f "\$backup_file" ]; then
                            cp "\$backup_file" /dest/
                            echo "[OK] Copied \$(basename \$backup_file)"
                        fi
                    done

                    # Copy kubeconfig if it exists
                    if [ -f /source/kubeconfig.yaml ]; then
                        cp /source/kubeconfig.yaml /dest/
                        echo "[OK] Copied kubeconfig.yaml"
                    elif [ -f /source/group_vars/kubeconfig.yaml ]; then
                        cp /source/group_vars/kubeconfig.yaml /dest/
                        echo "[OK] Copied kubeconfig.yaml from group_vars"
                    else
                        echo "[WARN] kubeconfig.yaml not found in shared volume"
                    fi

                    # Copy rendered group_vars if they exist
                    if [ -f /source/group_vars/all.yml ]; then
                        mkdir -p /dest/group_vars
                        cp /source/group_vars/all.yml /dest/group_vars/all.yml
                        echo "[OK] Copied group_vars/all.yml"
                    else
                        echo "[WARN] group_vars/all.yml not found in shared volume"
                    fi

                    # List what we successfully copied
                    echo "Files successfully copied to Jenkins workspace:"
                    ls -la /dest/*.json /dest/*.yml /dest/group_vars/*.yml /dest/*.tfvars /dest/*.tfstate 2>/dev/null || echo "No matching files found"
                '
        """

        // Generate deployment summary if it doesn't exist
        generateDeploymentSummary()

        logInfo('Artifact extraction completed successfully')
    } catch (Exception e) {
        logError("Artifact extraction failed: ${e.message}")
        // Don't fail the build, just log the issue
        logWarning('Build will continue, but some artifacts may not be available for archival')
    }
}

def generateDeploymentSummary() {
    logInfo('Generating deployment summary')

    try {
        def timestamp = new Date().format('yyyy-MM-dd HH:mm:ss')
        def summary = [
            deployment_info: [
                timestamp: timestamp,
                build_number: env.BUILD_NUMBER,
                job_name: env.JOB_NAME,
                workspace: env.TF_WORKSPACE,
                rke2_version: env.RKE2_VERSION,
                rancher_version: env.RANCHER_VERSION,
                rancher_hostname: env.RANCHER_HOSTNAME
            ],
            infrastructure: [
                terraform_vars_file: env.TERRAFORM_VARS_FILENAME,
                s3_bucket: env.S3_BUCKET_NAME,
                s3_region: env.S3_REGION,
                hostname_prefix: env.HOSTNAME_PREFIX
            ],
            artifacts_generated: []
        ]

        // Check which artifacts were successfully generated
        def artifactFiles = [
            'infrastructure-outputs.json',
            'ansible-inventory.yml',
            env.TERRAFORM_VARS_FILENAME,
            'terraform.tfstate'
        ]

        artifactFiles.each { fileName ->
            if (fileExists(fileName)) {
                summary.artifacts_generated.add(fileName)
            }
        }

        def summaryJson = groovy.json.JsonOutput.toJson(summary)
        writeFile file: 'deployment-summary.json', text: groovy.json.JsonOutput.prettyPrint(summaryJson)

        logInfo('Deployment summary generated successfully')
    } catch (Exception e) {
        logError("Failed to generate deployment summary: ${e.message}")
        logError("Exception details: ${e}")
    }
}

// ========================================
// INFRASTRUCTURE DEPLOYMENT FUNCTIONS
// ========================================

def generateTofuConfiguration() {
    logInfo('Generating Terraform configuration')

    if (!env.TERRAFORM_CONFIG) {
        error('TERRAFORM_CONFIG environment variable is not set')
    }

    // Ensure S3 backend parameters are set
    if (!env.S3_BUCKET_NAME) { error('S3_BUCKET_NAME environment variable is not set') }
    if (!env.S3_REGION) { error('S3_REGION environment variable is not set') }
    if (!env.S3_KEY_PREFIX) { error('S3_KEY_PREFIX environment variable is not set') }

    sh 'mkdir -p qa-infra-automation/tofu/aws/modules/airgap'

    def terraformConfig = env.TERRAFORM_CONFIG

    // Replace variables in config (similar to Jenkinsfile.recurring pattern)
    terraformConfig = terraformConfig.replace('${AWS_SECRET_ACCESS_KEY}', env.AWS_SECRET_ACCESS_KEY ?: '')
    terraformConfig = terraformConfig.replace('${AWS_ACCESS_KEY_ID}', env.AWS_ACCESS_KEY_ID ?: '')
    terraformConfig = terraformConfig.replace('${HOSTNAME_PREFIX}', env.HOSTNAME_PREFIX ?: '')

    // Write the configuration file
    dir('./qa-infra-automation') {
        dir('./tofu/aws/modules/airgap') {
            writeFile file: env.TERRAFORM_VARS_FILENAME, text: terraformConfig
            logInfo("Terraform configuration written to: ${env.TERRAFORM_VARS_FILENAME}")

            // Create proper backend.tf file with S3 configuration
            def backendConfig = """
terraform {
  backend "s3" {
    bucket = "${env.S3_BUCKET_NAME}"
    key    = "${env.S3_KEY_PREFIX}"
    region = "${env.S3_REGION}"
  }
}
"""
            writeFile file: env.TERRAFORM_BACKEND_CONFIG_FILENAME, text: backendConfig
            logInfo("S3 backend configuration written to: ${env.TERRAFORM_BACKEND_CONFIG_FILENAME}")
        }
    }
}