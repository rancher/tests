#!/usr/bin/env groovy

/**
 * Infrastructure Destruction Jenkinsfile for Airgap RKE2
 *
 * This pipeline is designed to safely destroy infrastructure created by
 * the main airgap RKE2 deployment pipeline. It retrieves Terraform state
 * from S3 backend and performs controlled destruction.
 */

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 2, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
    }

    parameters {
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: 'https://github.com/rancher/tests',
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: 'https://github.com/rancher/qa-infra-automation',
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'S3_BUCKET_NAME',
            defaultValue: 'jenkins-terraform-state-storage',
            description: 'S3 bucket name where Terraform state is stored'
        )
        string(
            name: 'S3_REGION',
            defaultValue: 'us-east-2',
            description: 'AWS region where the S3 bucket is located'
        )
        string(
            name: 'S3_KEY_PREFIX',
            defaultValue: 'jenkins-airgap-rke2',
            description: 'S3 key prefix for the Terraform state files'
        )
        string(
            name: 'TARGET_WORKSPACE',
            defaultValue: '',
            description: 'Terraform workspace to destroy (e.g., jenkins_airgap_ansible_workspace_123)'
        )
        choice(
            name: 'STATE_SOURCE',
            choices: ['S3_BACKEND'],
            description: 'Terraform state source (S3 backend only)'
        )
        booleanParam(
            name: 'FORCE_DESTROY',
            defaultValue: false,
            description: 'Force destruction even if state validation fails'
        )
        booleanParam(
            name: 'DRY_RUN',
            defaultValue: true,
            description: 'Perform dry run (plan only) without actual destruction'
        )
        choice(
            name: 'LOG_LEVEL',
            choices: ['INFO', 'DEBUG', 'VERBOSE'],
            description: 'Pipeline logging level'
        )
    }

    environment {
        // Dynamic state storage configuration from parameters
        TF_STATE_BUCKET = "${params.S3_BUCKET_NAME ?: 'jenkins-terraform-state-storage'}"
        TF_STATE_KEY_PREFIX = "${params.S3_KEY_PREFIX ?: 'jenkins-airgap-rke2'}"
        TF_STATE_REGION = "${params.S3_REGION ?: 'us-east-2'}"

        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: 'https://github.com/rancher/tests'}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: 'https://github.com/rancher/qa-infra-automation'}"
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${JOB_SHORT_NAME}${BUILD_NUMBER}-destroy"
        IMAGE_NAME = "rancher-destroy-${JOB_SHORT_NAME}${BUILD_NUMBER}"
        VALIDATION_VOLUME = "DestroySharedVolume-${JOB_SHORT_NAME}${BUILD_NUMBER}"

        // Target workspace from parameters
        TARGET_WORKSPACE = "${params.TARGET_WORKSPACE}"

        // Configuration files
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        ENV_FILE = '.env-destroy'

        // AWS defaults
        AWS_REGION = 'us-east-2'
    }

    stages {
        stage('Infrastructure Destruction Operations') {
            steps {
                script {
                    logWarning('‚ö†Ô∏è IMPORTANT: No concurrent state locking available')
                    logWarning('‚ö†Ô∏è Ensure only ONE destruction pipeline runs at a time for the same workspace')
                    logWarning('‚ö†Ô∏è Manual coordination required to prevent state conflicts')
                }

                    // Validate Parameters
                    script {
                        logInfo('Validating destruction parameters')

                        if (!params.TARGET_WORKSPACE || params.TARGET_WORKSPACE.trim().isEmpty()) {
                            error('TARGET_WORKSPACE parameter is required for destruction')
                        }

                        env.TARGET_WORKSPACE = params.TARGET_WORKSPACE

                        logInfo("Target workspace: ${env.TARGET_WORKSPACE}")
                        logInfo('State source: S3 Backend')
                        logInfo("Dry run mode: ${params.DRY_RUN}")
                        logInfo("Force destroy: ${params.FORCE_DESTROY}")
                    }

                    // Prepare Environment
                    script {
                        logInfo('Preparing destruction environment')

                        // Clean workspace
                        deleteDir()

                        // Checkout QA infrastructure repository
                        dir('./qa-infra-automation') {
                            logInfo('Cloning qa-infra-automation repository')
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.QA_INFRA_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[url: env.QA_INFRA_REPO]]
                            ])
                        }

                        // Checkout Tests repository for Dockerfile access
                        dir('./tests') {
                            logInfo('Cloning tests repository for Docker build')
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.RANCHER_TEST_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[url: env.RANCHER_TEST_REPO_URL,]]
                            ])
                        }

                        // Configure credentials and build environment
                        withCredentials(getCredentialsList()) {
                            generateDestructionEnvironmentFile()
                            setupSSHKeys()
                            buildDestructionDockerImage()
                            createSharedVolume()
                        }
                    }

                    // Retrieve Terraform State
                    script {
                        logInfo('Retrieving Terraform state from S3 backend for destruction')

                        def stateRetrieved = retrieveStateFromS3()

                        if (!stateRetrieved && !params.FORCE_DESTROY) {
                            error('Could not retrieve Terraform state from S3 and FORCE_DESTROY is not enabled')
                        } else if (!stateRetrieved && params.FORCE_DESTROY) {
                            logWarning('S3 state retrieval failed but FORCE_DESTROY is enabled - proceeding with caution')
                        }

                        // Validate state if retrieved
                        if (stateRetrieved) {
                            validateStateForDestruction()
                        }
                    }

                    // Plan Destruction
                    script {
                        logInfo('Planning infrastructure destruction')

                        try {
                            planDestruction()

                            // Archive the destruction plan
                            archiveDestructionPlan()

                            if (params.DRY_RUN) {
                                logInfo('DRY_RUN mode enabled - destruction plan completed without execution')
                                currentBuild.result = 'SUCCESS'
                                return
                            }
                        } catch (Exception e) {
                            logError("Destruction planning failed: ${e.message}")
                            throw e
                        }
                    }

                    // Execute Destruction (only if not DRY_RUN)
                    script {
                        if (!params.DRY_RUN) {
                            logInfo('Executing infrastructure destruction')

                            def proceedWithDestruction = true

                            // Final confirmation step
                            if (!params.FORCE_DESTROY) {
                                timeout(time: 5, unit: 'MINUTES') {
                                    try {
                                        proceedWithDestruction = input(
                                            message: "Proceed with destruction of workspace: ${env.TARGET_WORKSPACE}?",
                                            ok: 'Destroy',
                                            parameters: [
                                                booleanParam(
                                                    name: 'CONFIRM_DESTRUCTION',
                                                    defaultValue: false,
                                                    description: 'I understand this will permanently destroy the infrastructure'
                                                )
                                            ]
                                        )
                                    } catch (Exception e) {
                                        logWarning('Destruction cancelled by user or timeout')
                                        currentBuild.result = 'ABORTED'
                                        return
                                    }
                                }
                            }

                            if (proceedWithDestruction || params.FORCE_DESTROY) {
                                executeDestruction()
                                validateDestruction()
                                logInfo('Infrastructure destruction completed successfully')
                                archiveDestructionResults()
                            } else {
                                logInfo('Destruction cancelled by user')
                                currentBuild.result = 'ABORTED'
                            }
                        }
                    }
            }
            post {
                failure {
                    script {
                        logError('Infrastructure destruction operations failed')
                        archiveDestructionFailureArtifacts()
                    }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-destruction cleanup')

                // Archive important artifacts
                archiveBuildArtifacts([
                    'destruction-plan.txt',
                    'destruction-summary.json',
                    'destruction-logs.txt'
                ])

                // Cleanup containers and volumes
                cleanupContainersAndVolumes()
            }
        }

        success {
            script {
                logInfo('Destruction pipeline completed successfully')
                sendSlackNotification([
                    color: 'good',
                    message: "‚úÖ Infrastructure destruction succeeded for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        failure {
            script {
                logError('Destruction pipeline failed')
                sendSlackNotification([
                    color: 'danger',
                    message: "‚ùå Infrastructure destruction failed for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }

        aborted {
            script {
                logWarning('Destruction pipeline was aborted')
                sendSlackNotification([
                    color: 'warning',
                    message: "‚ö†Ô∏è Infrastructure destruction aborted for workspace: ${env.TARGET_WORKSPACE}"
                ])
            }
        }
    }
}

/**
 * DESTRUCTION-SPECIFIC HELPER FUNCTIONS
 */

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'SLACK_WEBHOOK', variable: 'SLACK_WEBHOOK')
    ]
}

def generateDestructionEnvironmentFile() {
    logInfo('Generating environment file for destruction containers')

    // Build environment content securely without direct interpolation of secrets
    def envLines = [
        '# Environment variables for infrastructure destruction containers',
        "TARGET_WORKSPACE=${env.TARGET_WORKSPACE}",
        "BUILD_NUMBER=${env.BUILD_NUMBER}",
        "JOB_NAME=${env.JOB_NAME}",
        "QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}",
        "TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}",
        "TF_STATE_BUCKET=${env.TF_STATE_BUCKET}",
        "TF_STATE_KEY_PREFIX=${env.TF_STATE_KEY_PREFIX}",
        "TF_STATE_REGION=${env.TF_STATE_REGION}",
        "AWS_REGION=${env.AWS_REGION}",
        '',
        '# AWS Credentials for OpenTofu'
    ]

    // Add credentials securely
    envLines.add('AWS_ACCESS_KEY_ID=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('AWS_SECRET_ACCESS_KEY=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('')
    envLines.add('# Terraform Variables for OpenTofu (TF_VAR_ prefix for automatic variable population)')
    envLines.add('TF_VAR_aws_access_key=' + env.AWS_ACCESS_KEY_ID)
    envLines.add('TF_VAR_aws_secret_access_key=' + env.AWS_SECRET_ACCESS_KEY)
    envLines.add('TF_VAR_aws_region=' + env.AWS_REGION)

    def envContent = envLines.join('\n')
    writeFile file: env.ENV_FILE, text: envContent
    logInfo("Environment file created: ${env.ENV_FILE}")
}

def setupSSHKeys() {
    if (env.AWS_SSH_PEM_KEY && env.AWS_SSH_KEY_NAME) {
        logInfo('Setting up SSH keys')

        dir('./tests/.ssh') {
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
        }

        logInfo('SSH keys configured successfully')
    }
}

def buildDestructionDockerImage() {
    logInfo("Building destruction Docker image: ${env.IMAGE_NAME}")

    dir('./') {
        sh './tests/validation/configure.sh || echo "Configure script not found, continuing..."'
        sh """
            # Use the same Dockerfile as the main pipeline for consistency
            docker build . \
                -f ./tests/validation/Dockerfile.tofu.e2e \
                -t ${env.IMAGE_NAME} \
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
                --build-arg VCS_REF=\$(git rev-parse --short HEAD 2>/dev/null || echo 'unknown') \
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \
                --label "pipeline.job.name=${env.JOB_NAME}" \
                --label "pipeline.purpose=destruction"
        """
    }

    logInfo('Destruction Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def executeScriptInContainer(scriptContent, extraEnv = [:]) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    // Note: TF_WORKSPACE is intentionally NOT set here to avoid workspace conflicts during init
    // The script itself will manage workspace selection after backend initialization
    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def executeScriptInContainerWithWorkspace(scriptContent, extraEnv = [:]) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    // This version sets TF_WORKSPACE for operations that need it (like planning and applying)
    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TARGET_WORKSPACE} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def retrieveStateFromS3() {
    logInfo("Enhanced state retrieval with alternative S3 key pattern detection")

    try {
        def retrieveScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'üîç Enhanced State Retrieval for Workspace: ${env.TARGET_WORKSPACE}'
echo '=============================================================='
echo "S3 bucket: ${env.TF_STATE_BUCKET}"
echo "Key prefix: ${env.TF_STATE_KEY_PREFIX}"
echo "Region: ${env.TF_STATE_REGION}"
echo "Target workspace: ${env.TARGET_WORKSPACE}"
echo

# Step 1: Use correct S3 key pattern
echo 'Step 1: Using correct S3 key pattern...'
mkdir -p tofu/aws/modules/airgap/

# Use the correct S3 key pattern: prefix/workspace/terraform.tfstate
ACTUAL_STATE_KEY="${env.TF_STATE_KEY_PREFIX}/${env.TARGET_WORKSPACE}/terraform.tfstate"

echo "üéØ Using confirmed S3 key pattern: \$ACTUAL_STATE_KEY"
echo "Full S3 path: s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY"

# Verify the exact state file exists
echo "üîç Verifying state file exists at confirmed location..."
if aws s3 ls "s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} >/dev/null 2>&1; then
    echo "‚úÖ CONFIRMED: State file exists at exact location"
    
    # Get file metadata
    STATE_METADATA=\$(aws s3api head-object --bucket "${env.TF_STATE_BUCKET}" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} 2>/dev/null)
    STATE_SIZE=\$(echo "\$STATE_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('ContentLength', 0))" 2>/dev/null || echo 0)
    LAST_MODIFIED=\$(echo "\$STATE_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('LastModified', 'unknown'))" 2>/dev/null || echo "unknown")
    
    echo "State file details:"
    echo "  üìè Size: \$STATE_SIZE bytes"
    echo "  üïê Modified: \$LAST_MODIFIED"
    
    if [ "\$STATE_SIZE" -eq 0 ]; then
        echo "‚ö†Ô∏è WARNING: State file is empty (0 bytes)"
        echo "This indicates one of the following scenarios:"
        echo "  1. Deployment failed after creating state file"
        echo "  2. Infrastructure was already destroyed"
        echo "  3. State file was corrupted or truncated"
        echo
        
        # Check for S3 versioning to find previous state versions
        echo "üîç Checking for previous state file versions..."
        VERSIONS=\$(aws s3api list-object-versions \
            --bucket "${env.TF_STATE_BUCKET}" \
            --prefix "\$ACTUAL_STATE_KEY" \
            --region ${env.TF_STATE_REGION} \
            --query 'Versions[?Size > `0`].[VersionId,Size,LastModified]' \
            --output table 2>/dev/null || echo "")
        
        if [ -n "\$VERSIONS" ] && [ "\$VERSIONS" != "None" ]; then
            echo "‚úÖ Found previous non-empty state versions:"
            echo "\$VERSIONS"
            
            # Get the latest non-empty version
            LATEST_VERSION=\$(aws s3api list-object-versions --bucket "${env.TF_STATE_BUCKET}" --prefix "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} --query 'Versions[?Size > `0`] | sort_by(@, &LastModified) | [-1].VersionId' --output text 2>/dev/null || echo "")
            
            if [ -n "\$LATEST_VERSION" ] && [ "\$LATEST_VERSION" != "None" ]; then
                echo "üîÑ Attempting to restore from latest non-empty version: \$LATEST_VERSION"
                
                if aws s3api copy-object --bucket "${env.TF_STATE_BUCKET}" --copy-source "${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY?versionId=\$LATEST_VERSION" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} >/dev/null 2>&1; then
                    echo "‚úÖ State file restored from version \$LATEST_VERSION"
                    
                    # Re-check the restored file
                    RESTORED_METADATA=\$(aws s3api head-object --bucket "${env.TF_STATE_BUCKET}" --key "\$ACTUAL_STATE_KEY" --region ${env.TF_STATE_REGION} 2>/dev/null)
                    RESTORED_SIZE=\$(echo "\$RESTORED_METADATA" | python3 -c "import json,sys; data=json.load(sys.stdin); print(data.get('ContentLength', 0))" 2>/dev/null || echo 0)
                    
                    if [ "\$RESTORED_SIZE" -gt 0 ]; then
                        echo "‚úÖ Restored state file size: \$RESTORED_SIZE bytes"
                        STATE_SIZE=\$RESTORED_SIZE
                    else
                        echo "‚ùå Restored state file is still empty"
                    fi
                else
                    echo "‚ùå Failed to restore state from version \$LATEST_VERSION"
                fi
            fi
        else
            echo "‚ùå No previous non-empty state versions found"
        fi
        
        # If still empty after restoration attempt
        if [ "\$STATE_SIZE" -eq 0 ]; then
            echo
            echo "üîç Checking for orphaned AWS resources..."
            echo "Searching for resources that might exist despite empty state:"
            
            # Check for common resource patterns that might exist
            echo "Checking for EC2 instances with workspace tag..."
            INSTANCES=\$(aws ec2 describe-instances --region ${env.TF_STATE_REGION} --filters "Name=tag:workspace,Values=${env.TARGET_WORKSPACE}" --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==`Name`].Value|[0]]' --output table 2>/dev/null || echo "No instances found")
            
            if [ "\$INSTANCES" != "No instances found" ] && [ -n "\$INSTANCES" ]; then
                echo "‚ö†Ô∏è Found EC2 instances that may be orphaned:"
                echo "\$INSTANCES"
                ORPHANED_RESOURCES=true
            else
                echo "No EC2 instances found with workspace tag"
            fi
            
            echo "Checking for VPCs with workspace tag..."
            VPCS=\$(aws ec2 describe-vpcs --region ${env.TF_STATE_REGION} --filters "Name=tag:workspace,Values=${env.TARGET_WORKSPACE}" --query 'Vpcs[*].[VpcId,State,Tags[?Key==`Name`].Value|[0]]' --output table 2>/dev/null || echo "No VPCs found")
            
            if [ "\$VPCS" != "No VPCs found" ] && [ -n "\$VPCS" ]; then
                echo "‚ö†Ô∏è Found VPCs that may be orphaned:"
                echo "\$VPCS"
                ORPHANED_RESOURCES=true
            else
                echo "No VPCs found with workspace tag"
            fi
            
            echo
            if [ "\$ORPHANED_RESOURCES" = "true" ]; then
                echo "‚ö†Ô∏è ORPHANED RESOURCES DETECTED"
                echo "Found AWS resources that may belong to this workspace but are not in Terraform state."
                echo "These resources may need manual cleanup."
                echo
                echo "Recommended actions:"
                echo "1. Review the resources listed above"
                echo "2. Manually delete orphaned resources if confirmed"
                echo "3. Check deployment logs for what went wrong"
                echo
                if [ "${params.FORCE_DESTROY ?: 'false'}" = "true" ]; then
                    echo "‚ö†Ô∏è FORCE_DESTROY is enabled, but no state resources to destroy"
                    echo "Manual cleanup of orphaned resources may be required"
                    exit 0
                else
                    echo "‚ùå Empty state with potential orphaned resources - manual intervention required"
                    exit 1
                fi
            else
                echo "‚úÖ No orphaned resources found - infrastructure appears to be fully cleaned up"
                echo "The empty state file indicates successful cleanup or deployment failure"
                echo
                if [ "${params.FORCE_DESTROY ?: 'false'}" = "true" ]; then
                    echo "‚úÖ FORCE_DESTROY enabled - treating empty state as successful cleanup"
                    exit 0
                else
                    echo "‚ÑπÔ∏è No resources to destroy - workspace appears clean"
                    echo "This may indicate:"
                    echo "  - Infrastructure was already destroyed"
                    echo "  - Original deployment never completed"
                    echo "  - This is the expected state"
                    exit 0
                fi
            fi
        fi
    fi
    
else
    echo "‚ùå ERROR: State file not found at confirmed location"
    echo "Expected: s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY"
    
    # Fallback: search for any files with this workspace name
    echo "üîç Searching for alternative locations..."
    SEARCH_RESULTS=\$(aws s3 ls s3://${env.TF_STATE_BUCKET}/ --region ${env.TF_STATE_REGION} --recursive | grep "${env.TARGET_WORKSPACE}" | grep "terraform.tfstate" || echo "")
    
    if [ -n "\$SEARCH_RESULTS" ]; then
        echo "Found state files matching workspace pattern:"
        echo "\$SEARCH_RESULTS"
        
        # Use the first match as fallback
        ACTUAL_STATE_KEY=\$(echo "\$SEARCH_RESULTS" | head -1 | awk '{print \$4}')
        STATE_SIZE=\$(echo "\$SEARCH_RESULTS" | head -1 | awk '{print \$3}')
        echo "Using fallback state file: \$ACTUAL_STATE_KEY"
        
        if [ "\$STATE_SIZE" -eq 0 ]; then
            echo "‚ùå ERROR: Fallback state file is also empty"
            exit 1
        fi
    else
        echo "‚ùå ERROR: No state files found for workspace ${env.TARGET_WORKSPACE}"
        exit 1
    fi
fi

# Step 2: Configure backend with actual discovered S3 key
echo
echo 'Step 2: Configuring backend with discovered S3 key...'

cat > tofu/aws/modules/airgap/backend.tf << EOF
terraform {
  backend "s3" {
    bucket  = "${env.TF_STATE_BUCKET}"
    key     = "\$ACTUAL_STATE_KEY"
    region  = "${env.TF_STATE_REGION}"
    encrypt = true
    
    # Enhanced backend settings for reliability
    skip_credentials_validation = false
    skip_metadata_api_check     = false
    skip_region_validation      = false
    skip_requesting_account_id  = false
    
    # Note: No DynamoDB locking configured
    # Using direct S3 key access (no workspace needed)
  }
}
EOF

echo "‚úÖ Backend configuration created with discovered key:"
cat tofu/aws/modules/airgap/backend.tf

# Step 3: Initialize OpenTofu with direct S3 key backend
echo
echo 'Step 3: Initializing OpenTofu with direct S3 key access...'

# CRITICAL: Unset TF_WORKSPACE for direct state access
unset TF_WORKSPACE

# Clean any existing backend state
rm -f tofu/aws/modules/airgap/.terraform/terraform.tfstate
rm -rf tofu/aws/modules/airgap/.terraform/terraform.tfstate.d/
rm -f tofu/aws/modules/airgap/.terraform.lock.hcl

# Initialize backend with direct S3 key
if tofu -chdir=tofu/aws/modules/airgap init -input=false -reconfigure; then
    echo "‚úÖ OpenTofu backend initialized successfully with direct S3 key"
else
    echo "‚ùå ERROR: Failed to initialize OpenTofu backend"
    
    # Debug initialization failure
    echo "Debugging backend initialization failure:"
    echo "1. Checking AWS credentials:"
    aws sts get-caller-identity || echo "AWS credentials check failed"
    
    echo "2. Checking S3 bucket access:"
    aws s3 ls s3://${env.TF_STATE_BUCKET}/ --region ${env.TF_STATE_REGION} || echo "S3 bucket access failed"
    
    echo "3. Checking backend configuration:"
    cat tofu/aws/modules/airgap/backend.tf || echo "Backend config missing"
    
    echo "4. Testing direct state file access:"
    aws s3 ls s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY --region ${env.TF_STATE_REGION} || echo "Direct state file access failed"
    
    exit 1
fi

# Step 4: Verify state resources are loaded (no workspace needed with direct key)
echo
echo 'Step 4: Verifying state resources are loaded...'
LOADED_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
echo "Resources loaded from state: \$LOADED_RESOURCES"

if [ "\$LOADED_RESOURCES" -eq 0 ]; then
    echo "‚ö†Ô∏è WARNING: No resources found in state"
    echo "This could mean:"
    echo "  1. Infrastructure was already destroyed"
    echo "  2. State file is empty or corrupted"
    echo "  3. Configuration compatibility issues"
    
    echo
    echo "üîç Additional debugging:"
    echo "Direct state file download test:"
    
    # Try to download and analyze state directly
    TEMP_STATE="/tmp/direct_state_analysis.tfstate"
    if aws s3 cp s3://${env.TF_STATE_BUCKET}/\$ACTUAL_STATE_KEY "\$TEMP_STATE" --region ${env.TF_STATE_REGION} --quiet; then
        echo "‚úÖ State file downloaded successfully"
        
        # Check if state file is empty first
        STATE_FILE_SIZE=\$(stat -c%s "\$TEMP_STATE" 2>/dev/null || echo 0)
        echo "State file size: \$STATE_FILE_SIZE bytes"

        if [ "\$STATE_FILE_SIZE" -eq 0 ]; then
            echo "‚ÑπÔ∏è State file is empty - no resources to destroy"
        elif python3 -c "import json; json.load(open('\$TEMP_STATE'))" 2>/dev/null; then
            DIRECT_RESOURCES=\$(python3 -c "import json; data=json.load(open('\$TEMP_STATE')); print(len(data.get('resources', [])))" 2>/dev/null || echo 0)
            echo "Resources in direct state file: \$DIRECT_RESOURCES"

            if [ "\$DIRECT_RESOURCES" -gt 0 ]; then
                echo "‚ö†Ô∏è State file contains resources but OpenTofu cannot access them"
                echo "This may indicate configuration or provider compatibility issues"
            else
                echo "‚ÑπÔ∏è State file is valid JSON but contains no resources - infrastructure already destroyed"
            fi
        else
            echo "‚ö†Ô∏è State file is not valid JSON, but OpenTofu can still process it"
            echo "This is normal for some Terraform state formats or partially initialized states"
            echo "Since destruction planning works, the state is accessible to OpenTofu"
        fi
        
        rm -f "\$TEMP_STATE"
    else
        echo "‚ùå Failed to download state file directly"
    fi
    
    if [ "${params.FORCE_DESTROY ?: 'false'}" != "true" ]; then
        echo "‚ùå No resources to destroy and FORCE_DESTROY not enabled"
        exit 1
    else
        echo "‚ö†Ô∏è FORCE_DESTROY enabled, continuing despite no resources"
    fi
else
    echo "‚úÖ Found \$LOADED_RESOURCES resources ready for destruction"
    echo
    echo "Sample resources in state (first 10):"
    tofu -chdir=tofu/aws/modules/airgap state list | head -10 | while read -r resource; do
        echo "  ‚Üí \$resource"
    done
fi

echo
echo "‚úÖ SUCCESS: Enhanced state retrieval completed"
echo "   - Backend: S3 Direct Key Access"
echo "   - State file: \$ACTUAL_STATE_KEY"
echo "   - File size: \$STATE_SIZE bytes"
echo "   - Resources available: \$LOADED_RESOURCES"
echo "   - Ready for destruction planning"

# Store the actual state key for use in other functions
echo "\$ACTUAL_STATE_KEY" > /tmp/actual_state_key.txt
"""

        executeScriptInContainer(retrieveScript)
        logInfo('‚úÖ Enhanced state retrieval successful')
        return true
    } catch (Exception e) {
        logError("‚ùå Enhanced state retrieval failed: ${e.message}")
        
        // Enhanced troubleshooting with fallback options
        logError("üí° Enhanced troubleshooting suggestions:")
        logError("   1. State file found but with non-standard S3 key pattern")
        logError("   2. Run diagnostic script: ./scripts/diagnose_destroy_state_issue.sh ${env.TARGET_WORKSPACE}")
        logError("   3. Use targeted fix script: ./scripts/fix_destroy_state_mismatch.sh ${env.TARGET_WORKSPACE}")
        logError("   4. Verify deployment pipeline completed successfully")
        logError("   5. Check deployment logs for actual workspace name and S3 locations")
        
        return false
    }
}

def validateStateForDestruction() {
    logInfo('Validating state for destruction')

    try {
        def validationScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Validating Terraform configuration...'
tofu -chdir=tofu/aws/modules/airgap validate

echo 'Checking state resources...'
RESOURCE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
echo "Resources to be destroyed: \$RESOURCE_COUNT"

if [ "\$RESOURCE_COUNT" -eq 0 ]; then
    echo 'WARNING: No resources found in state'
    exit 1
fi

echo 'Listing resources to be destroyed:'
tofu -chdir=tofu/aws/modules/airgap state list

echo 'State validation completed successfully'
"""

        executeScriptInContainer(validationScript)
        logInfo('State validation completed')
    } catch (Exception e) {
        logWarning("State validation failed: ${e.message}")
        if (!params.FORCE_DESTROY) {
            throw new RuntimeException('State validation failed and FORCE_DESTROY is not enabled', e)
        }
    }
}

def planDestruction() {
    logInfo('Creating destruction plan with proper workspace environment')

    def planScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'üéØ Creating Destruction Plan'
echo '=========================='
echo "Target workspace: ${env.TARGET_WORKSPACE}"

# Select the target workspace before attempting destruction
echo "Selecting target workspace: ${env.TARGET_WORKSPACE}"
echo "Available workspaces:"
tofu -chdir=tofu/aws/modules/airgap workspace list

if tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TARGET_WORKSPACE} 2>/dev/null; then
    echo "‚úÖ Successfully selected workspace: ${env.TARGET_WORKSPACE}"
else
    echo "‚ùå ERROR: Could not select workspace: ${env.TARGET_WORKSPACE}"
    echo "This workspace may not exist or may have been deleted"
    echo "Available workspaces:"
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

# Verify we're in the correct workspace
CURRENT_WORKSPACE=\$(tofu -chdir=tofu/aws/modules/airgap workspace show | xargs)
echo "Active workspace: \$CURRENT_WORKSPACE"

if [ "\$CURRENT_WORKSPACE" != "${env.TARGET_WORKSPACE}" ]; then
    echo "‚ùå ERROR: Workspace selection failed"
    echo "Expected: ${env.TARGET_WORKSPACE}"
    echo "Current: \$CURRENT_WORKSPACE"
    exit 1
fi

echo 'Step 1: Attempting to download original terraform variables from S3...'
# Try to download the original tfvars file used during deployment
TFVARS_S3_KEY="${env.TF_STATE_KEY_PREFIX}/${env.TARGET_WORKSPACE}/${env.TERRAFORM_VARS_FILENAME}"
echo "Checking for original tfvars at: s3://${env.TF_STATE_BUCKET}/\$TFVARS_S3_KEY"

if aws s3 cp s3://${env.TF_STATE_BUCKET}/\$TFVARS_S3_KEY tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME} --region ${env.TF_STATE_REGION} 2>/dev/null; then
    echo "‚úÖ SUCCESS: Downloaded original terraform variables from S3"
    echo "Original tfvars file contents:"
    head -10 tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME}
else
    echo "‚ö†Ô∏è  WARNING: Could not download original tfvars from S3, creating minimal variables"
    echo "This may cause destruction issues if custom variables were used during deployment"

    cat > tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME} << 'EOFVARS'
# Minimal configuration for destruction
# Values are read from state, so these are just placeholders
aws_region = "${env.AWS_REGION}"
aws_access_key = "placeholder-for-destroy"
aws_secret_access_key = "placeholder-for-destroy"

# Common variables that might be required by the configuration
instance_type = "t3.medium"
node_count = 1
availability_zones = ["${env.AWS_REGION}a", "${env.AWS_REGION}b"]
EOFVARS

    echo "‚úÖ Created minimal terraform variables as fallback"
fi

echo
echo 'Step 2: Attempting destruction planning...'

# First try without tfvars (OpenTofu should read everything from state)
if tofu -chdir=tofu/aws/modules/airgap plan -destroy -input=false -out=destroy-plan 2>/dev/null; then
    echo "‚úÖ Destruction plan created successfully (no tfvars needed)"
else
    echo "Planning without tfvars failed, trying with minimal tfvars..."
    
    if tofu -chdir=tofu/aws/modules/airgap plan -destroy -input=false -var-file=${env.TERRAFORM_VARS_FILENAME} -out=destroy-plan; then
        echo "‚úÖ Destruction plan created successfully (with minimal tfvars)"
    else
        echo "‚ùå ERROR: Destruction planning failed"
        echo
        echo "Debugging destruction planning failure:"
        echo "1. Checking current state resources:"
        tofu -chdir=tofu/aws/modules/airgap state list | head -10 || echo "Could not list state resources"
        
        echo "2. Checking configuration validation:"
        tofu -chdir=tofu/aws/modules/airgap validate || echo "Configuration validation failed"
        
        echo "3. Attempting plan without output file for detailed error:"
        tofu -chdir=tofu/aws/modules/airgap plan -destroy -input=false -var-file=${env.TERRAFORM_VARS_FILENAME} 2>&1 | head -20 || echo "Plan generation failed"
        
        echo "4. Checking required variables:"
        tofu -chdir=tofu/aws/modules/airgap plan -destroy -input=false -var-file=${env.TERRAFORM_VARS_FILENAME} 2>&1 | grep -i "required" || echo "No specific variable requirements found"
        
        exit 1
    fi
fi

# Step 3: Verify plan file was created
if [ ! -f tofu/aws/modules/airgap/destroy-plan ]; then
    echo "‚ùå ERROR: destroy-plan file was not created"
    echo "Contents of module directory:"
    ls -la tofu/aws/modules/airgap/
    exit 1
fi

PLAN_SIZE=\$(stat -c%s tofu/aws/modules/airgap/destroy-plan 2>/dev/null || echo 0)
echo "Plan file size: \$PLAN_SIZE bytes"

if [ "\$PLAN_SIZE" -eq 0 ]; then
    echo "‚ùå ERROR: Plan file is empty"
    exit 1
fi

echo
echo 'Step 4: Generating destruction plan summary...'
if tofu -chdir=tofu/aws/modules/airgap show -no-color destroy-plan > destruction-plan-summary.txt; then
    echo "‚úÖ Destruction plan summary generated successfully"
    
    echo "Plan summary (first 30 lines):"
    head -30 destruction-plan-summary.txt
    
    echo
    DESTROY_COUNT=\$(grep -c "will be destroyed" destruction-plan-summary.txt || echo 0)
    echo "Resources to be destroyed: \$DESTROY_COUNT"
    
    if [ "\$DESTROY_COUNT" -eq 0 ]; then
        echo "‚ö†Ô∏è WARNING: No resources marked for destruction in plan"
        echo "This could indicate:"
        echo "  1. Infrastructure was already destroyed"
        echo "  2. State file issues"
        echo "  3. Configuration mismatch"
        
        if [ "${params.FORCE_DESTROY ?: 'false'}" != "true" ]; then
            echo "‚ùå No resources to destroy and FORCE_DESTROY not enabled"
            exit 1
        else
            echo "‚ö†Ô∏è FORCE_DESTROY enabled, continuing despite no resources"
        fi
    else
        echo "‚úÖ Found \$DESTROY_COUNT resources scheduled for destruction"
    fi
    
else
    echo "‚ùå ERROR: Failed to generate destruction plan summary"
    exit 1
fi

echo
echo "‚úÖ Destruction plan created and validated successfully"
echo "   - Workspace: \$CURRENT_WORKSPACE"
echo "   - Resources to destroy: \$DESTROY_COUNT"
echo "   - Plan file size: \$PLAN_SIZE bytes"
"""

    executeScriptInContainerWithWorkspace(planScript)
}

def archiveDestructionPlan() {
    logInfo('Archiving destruction plan')

    try {
        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/destruction-plan-summary.txt ./destruction-plan.txt || true
        """

        archiveArtifacts artifacts: 'destruction-plan.txt', allowEmptyArchive: true

        logInfo('Destruction plan archived')
    } catch (Exception e) {
        logError("Failed to archive destruction plan: ${e.message}")
    }
}

def executeDestruction() {
    logInfo('Executing infrastructure destruction')

    def destroyScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Starting infrastructure destruction...'
echo "Target workspace: ${env.TARGET_WORKSPACE}"
echo "Timestamp: \$(date -u +%Y-%m-%dT%H:%M:%SZ)"

# Execute the destruction
tofu -chdir=tofu/aws/modules/airgap apply -auto-approve -input=false destroy-plan

echo 'Verifying destruction completion...'
REMAINING_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)

if [ "\$REMAINING_RESOURCES" -eq 0 ]; then
    echo 'SUCCESS: All resources have been destroyed'

    # Clean up the workspace
    echo 'Cleaning up Terraform workspace...'
    tofu -chdir=tofu/aws/modules/airgap workspace select default || echo 'Could not switch to default workspace'
    tofu -chdir=tofu/aws/modules/airgap workspace delete ${env.TARGET_WORKSPACE} || echo 'Could not delete workspace'

else
    echo "WARNING: \$REMAINING_RESOURCES resources still remain in state"
    echo 'Remaining resources:'
    tofu -chdir=tofu/aws/modules/airgap state list
    exit 1
fi

echo 'Infrastructure destruction completed successfully'
"""

    executeScriptInContainer(destroyScript)
}

def validateDestruction() {
    logInfo('Validating destruction results')

    try {
        def validationScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Final validation of destruction...'

# Check if any resources remain
RESOURCE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l || echo 0)

if [ "\$RESOURCE_COUNT" -eq 0 ]; then
    echo 'SUCCESS: Destruction validation passed - no resources remain'
else
    echo "WARNING: \$RESOURCE_COUNT resources still exist in state"
    tofu -chdir=tofu/aws/modules/airgap state list
fi

# Generate destruction summary
cat > destruction-summary.json << EOF
{
    "workspace": "${env.TARGET_WORKSPACE}",
    "destruction_timestamp": "\$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "build_number": "${env.BUILD_NUMBER}",
    "job_name": "${env.JOB_NAME}",
    "remaining_resources": \$RESOURCE_COUNT,
    "status": "\$([ \$RESOURCE_COUNT -eq 0 ] && echo 'COMPLETED' || echo 'INCOMPLETE')"
}
EOF

echo 'Destruction validation completed'
"""

        executeScriptInContainer(validationScript)
        logInfo('Destruction validation completed')
    } catch (Exception e) {
        logWarning("Destruction validation failed: ${e.message}")
    }
}

def archiveDestructionResults() {
    logInfo('Archiving destruction results')

    try {
        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/destruction-summary.json ./ || true
        """

        logInfo('Destruction results archived')
    } catch (Exception e) {
        logError("Failed to archive destruction results: ${e.message}")
    }
}

def archiveDestructionFailureArtifacts() {
    logInfo('Archiving destruction failure artifacts')

    try {
        def debugCommands = [
            "cd ${env.QA_INFRA_WORK_PATH}",
            'tofu -chdir=tofu/aws/modules/airgap workspace list > workspace-list.txt 2>&1 || echo "No workspace list available"',
            "tofu -chdir=tofu/aws/modules/airgap state list > remaining-resources.txt 2>&1 || echo 'No state available'",
            "echo 'Destruction failure artifact collection completed'"
        ]

        executeInContainer(debugCommands)

        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/workspace-list.txt ./ || true
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/remaining-resources.txt ./ || true
        """

        archiveArtifacts artifacts: 'workspace-list.txt,remaining-resources.txt', allowEmptyArchive: true
    } catch (Exception e) {
        logError("Failed to archive failure artifacts: ${e.message}")
    }
}

def executeInContainer(commands) {
    def commandString = commands.join(' && ')
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"
    def scriptFile = "destroy-commands-${timestamp}.sh"

    writeFile file: scriptFile, text: commandString

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TARGET_WORKSPACE} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def cleanupContainersAndVolumes() {
    logInfo('Cleaning up Docker containers and volumes')

    try {
        sh """
            # Stop and remove containers
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
            docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true

            # Remove the Docker image
            docker rmi -f ${env.IMAGE_NAME} || true

            # Remove the shared volume
            docker volume rm -f ${env.VALIDATION_VOLUME} || true

            # Clean up dangling resources
            docker system prune -f || true
        """
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }
}

def archiveBuildArtifacts(artifacts) {
    try {
        archiveArtifacts artifacts: artifacts.join(','), allowEmptyArchive: true
        logInfo("Artifacts archived: ${artifacts.join(', ')}")
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

def sendSlackNotification(config) {
    if (env.SLACK_WEBHOOK) {
        try {
            def payload = [
                channel: '#rancher-qa',
                username: 'Jenkins-Destroyer',
                color: config.color,
                title: 'Infrastructure Destruction Pipeline',
                message: config.message,
                fields: [
                    [title: 'Job', value: env.JOB_NAME, short: true],
                    [title: 'Build', value: env.BUILD_NUMBER, short: true],
                    [title: 'Workspace', value: env.TARGET_WORKSPACE, short: true],
                    [title: 'State Source', value: 'S3 Backend', short: true]
                ]
            ]

            httpRequest(
                httpMode: 'POST',
                url: env.SLACK_WEBHOOK,
                contentType: 'APPLICATION_JSON',
                requestBody: groovy.json.JsonOutput.toJson(payload)
            )

            logInfo('Slack notification sent successfully')
        } catch (Exception e) {
            logError("Failed to send Slack notification: ${e.message}")
        }
    }
}

/**
 * LOGGING FUNCTIONS
 */

def logInfo(message) {
    echo "‚ÑπÔ∏è [INFO] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logError(message) {
    echo "‚ùå [ERROR] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logWarning(message) {
    echo "‚ö†Ô∏è [WARNING] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logDebug(message) {
    if (params.LOG_LEVEL == 'DEBUG' || params.LOG_LEVEL == 'VERBOSE') {
        echo "üîç [DEBUG] ${new Date().format('HH:mm:ss')} - ${message}"
    }
}
