#!/usr/bin/env groovy

/**
 * Ansible Airgap Setup Jenkinsfile
 * Based on Jenkinsfile.recurring but adapted for airgap RKE2 infrastructure setup
 *
 * This pipeline sets up airgap RKE2 infrastructure using Ansible and OpenTofu
 * with enhanced error handling and proper workspace management.
 */

pipeline {
    agent any

    // Global pipeline options
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timeout(time: 3, unit: 'HOURS')
        timestamps()
        ansiColor('xterm')
        skipStagesAfterUnstable()
        retry(1)
    }

    // Environment-specific parameters
    parameters {
        string(
            name: 'RKE2_VERSION',
            defaultValue: 'v1.28.8+rke2r1',
            description: 'RKE2 version to deploy (e.g., v1.28.8+rke2r1, v1.29.5+rke2r1, v1.30.2+rke2r1)'
        )
        string(
            name: 'RANCHER_VERSION',
            defaultValue: 'v2.10-head',
            description: 'Rancher version to deploy (e.g., head, v2.10-head, v2.11.0, v2.9-head)'
        )
        string(
            name: 'RANCHER_TEST_REPO_URL',
            defaultValue: 'https://github.com/rancher/tests',
            description: 'URL of rancher/tests repository'
        )
        string(
            name: 'RANCHER_TEST_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of rancher/tests repository'
        )
        string(
            name: 'QA_INFRA_REPO_URL',
            defaultValue: 'https://github.com/rancher/qa-infra-automation',
            description: 'URL of qa-infra-automation repository'
        )
        string(
            name: 'QA_INFRA_REPO_BRANCH',
            defaultValue: 'main',
            description: 'Branch of qa-infra-automation repository'
        )
        string(
            name: 'PRIVATE_REGISTRY_URL',
            defaultValue: '',
            description: 'Private registry URL for airgap deployment'
        )
        string(
            name: 'PRIVATE_REGISTRY_USERNAME',
            defaultValue: '',
            description: 'Private registry username for airgap deployment'
        )
        password(
            name: 'PRIVATE_REGISTRY_PASSWORD',
            defaultValue: '',
            description: 'Private registry password for airgap deployment'
        )
        booleanParam(
            name: 'PARALLEL_EXECUTION',
            defaultValue: true,
            description: 'Enable parallel execution where possible'
        )
        booleanParam(
            name: 'DESTROY_ON_FAILURE',
            defaultValue: true,
            description: 'Destroy infrastructure when Ansible playbooks fail (automatic cleanup)'
        )
        choice(
            name: 'LOG_LEVEL',
            choices: ['INFO', 'DEBUG', 'VERBOSE'],
            description: 'Pipeline logging level'
        )
    }

    // Global environment variables
    environment {
        // Repository configurations
        RANCHER_TEST_REPO_URL = "${params.RANCHER_TEST_REPO_URL ?: 'https://github.com/rancher/tests'}"
        QA_INFRA_REPO = "${params.QA_INFRA_REPO_URL ?: 'https://github.com/rancher/qa-infra-automation'}"

        // Private registry configurations
        PRIVATE_REGISTRY_URL = "${params.PRIVATE_REGISTRY_URL ?: ''}"
        PRIVATE_REGISTRY_USERNAME = "${params.PRIVATE_REGISTRY_USERNAME ?: ''}"
        PRIVATE_REGISTRY_PASSWORD = "${params.PRIVATE_REGISTRY_PASSWORD ?: ''}"

        // Path configurations
        ROOT_PATH = '/root/go/src/github.com/rancher/tests/'
        QA_INFRA_WORK_PATH = '/root/go/src/github.com/rancher/qa-infra-automation'

        // Cleanup configurations
        DESTROY_ON_FAILURE = "${params.DESTROY_ON_FAILURE}"

        // Computed values
        JOB_SHORT_NAME = "${getShortJobName()}"
        BUILD_CONTAINER_NAME = "${JOB_SHORT_NAME}${BUILD_NUMBER}-airgap-ansible"
        IMAGE_NAME = "rancher-ansible-airgap-setup-${JOB_SHORT_NAME}${BUILD_NUMBER}"
        VALIDATION_VOLUME = "AnsibleAirgapSharedVolume-${JOB_SHORT_NAME}${BUILD_NUMBER}"

        // Configuration files
        ANSIBLE_VARS_FILENAME = 'vars.yaml'
        TERRAFORM_VARS_FILENAME = 'cluster.tfvars'
        ENV_FILE = '.env'

        // Terraform workspace
        TF_WORKSPACE = "jenkins_airgap_ansible_workspace_${BUILD_NUMBER}"

        // Timeouts (in minutes)
        TERRAFORM_TIMEOUT = '30'
        ANSIBLE_TIMEOUT = '45'
        VALIDATION_TIMEOUT = '15'

        // AWS Infrastructure defaults
        AWS_REGION = 'us-west-2'
        AWS_AMI = 'ami-0c2d3e23fb3adceb8'

        // State storage configuration
        TF_STATE_BUCKET = 'rancher-terraform-state-storage'
        TF_STATE_KEY_PREFIX = 'jenkins-airgap-rke2'
        TF_STATE_REGION = "${AWS_REGION}"
        JENKINS_STATE_ARCHIVE_PREFIX = 'terraform-state'
    }

    stages {
        stage('Initialize Pipeline') {
            steps {
                script {
                    // Validate parameters and environment
                    validateParameters()

                    // Set up dynamic variables
                    setupDynamicEnvironment()

                    // Clean workspace
                    deleteDir()

                    logInfo('Pipeline initialized successfully')
                    logInfo("Build container: ${env.BUILD_CONTAINER_NAME}")
                    logInfo("Docker image: ${env.IMAGE_NAME}")
                    logInfo("Volume: ${env.VALIDATION_VOLUME}")
                }
            }
            post {
                failure {
                    logError('Failed to initialize pipeline')
                }
            }
        }

        stage('Checkout Repositories') {
            parallel {
                stage('Checkout Rancher Tests') {
                    steps {
                        dir('./tests') {
                            logInfo("Cloning rancher tests repository from ${env.RANCHER_TEST_REPO_URL}")
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.RANCHER_TEST_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[
                                    url: env.RANCHER_TEST_REPO_URL,
                                ]]
                            ])
                        }
                    }
                }

                stage('Checkout QA Infrastructure') {
                    steps {
                        dir('./qa-infra-automation') {
                            logInfo("Cloning qa-infra-automation repository from ${env.QA_INFRA_REPO}")
                            checkout([
                                $class: 'GitSCM',
                                branches: [[name: "*/${params.QA_INFRA_REPO_BRANCH}"]],
                                extensions: [
                                    [$class: 'CleanCheckout'],
                                    [$class: 'CloneOption', depth: 1, shallow: true]
                                ],
                                userRemoteConfigs: [[
                                    url: env.QA_INFRA_REPO,
                                ]]
                            ])
                        }
                    }
                }
            }
            post {
                failure {
                    logError('Failed to checkout repositories')
                    cleanupContainersAndVolumes()
                }
            }
        }

        stage('Configure Environment') {
            steps {
                script {
                    logInfo('Configuring deployment environment')

                    // Configure credentials and environment files
                    withCredentials(getCredentialsList()) {
                        // Generate configuration files
                        generateConfigurationFiles()

                        // Setup SSH keys securely
                        setupSSHKeys()

                        // Build Docker image with proper tagging
                        buildDockerImage()

                        // Create shared volume
                        createSharedVolume()
                    }
                }
            }
            post {
                failure {
                    logError('Environment configuration failed')
                    cleanupContainersAndVolumes()
                }
            }
        }

        stage('Setup Infrastructure') {
            steps {
                script {
                    logInfo('Initializing airgap infrastructure deployment with OpenTofu')

                    // Configuration validation
                    def requiredVars = [
                        'QA_INFRA_WORK_PATH',
                        'TF_WORKSPACE',
                        'TERRAFORM_VARS_FILENAME',
                        'TERRAFORM_TIMEOUT'
                    ]

                    validateRequiredVariables(requiredVars)

                    // Enhanced timeout with reasonable defaults
                    def timeoutMinutes = env.TERRAFORM_TIMEOUT ?
                        Integer.parseInt(env.TERRAFORM_TIMEOUT) : 30

                    timeout(time: timeoutMinutes, unit: 'MINUTES') {
                        try {
                            // Pre-flight checks
                            validateInfrastructurePrerequisites()

                            // Infrastructure deployment with enhanced error handling
                            deployInfrastructure()

                            // Post-deployment validation
                            validateInfrastructureState()

                            logInfo('Infrastructure provisioned and validated successfully')
                        } catch (org.jenkinsci.plugins.workflow.steps.FlowInterruptedException e) {
                            logError("Infrastructure deployment timed out after ${timeoutMinutes} minutes")
                            handleInfrastructureFailure('TIMEOUT', e)
                            throw e
                        } catch (Exception e) {
                            logError("Infrastructure setup failed: ${e.message}")
                            handleInfrastructureFailure('DEPLOYMENT_FAILED', e)
                            throw e
                        }
                    }
                }
            }
            post {
                failure {
                    script {
                        logError('Infrastructure setup stage failed')
                        archiveInfrastructureFailureArtifacts()
                    }
                }
                always {
                    script {
                        archiveInfrastructureState()
                    }
                }
            }
        }
    }

    post {
        always {
            script {
                logInfo('Starting post-build cleanup')

                // Archive important artifacts
                archiveBuildArtifacts([
                    'kubeconfig.yaml',
                    'terraform.tfstate',
                    'ansible-logs.txt',
                    'deployment-summary.json'
                ])

                // Always cleanup containers and volumes
                try {
                    node {
                        cleanupContainersAndVolumes()
                    }
                } catch (Exception e) {
                    logError("Node context not available for cleanup: ${e.message}")
                    try {
                        cleanupContainersAndVolumes()
                    } catch (Exception cleanupException) {
                        logError("Cleanup failed: ${cleanupException.message}")
                    }
                }
            }
        }

        success {
            script {
                logInfo('Pipeline completed successfully')
                sendSlackNotification([
                    color: 'good',
                    message: "âœ… Ansible Airgap setup succeeded for ${env.JOB_NAME} #${env.BUILD_NUMBER}"
                ])
            }
        }

        failure {
            script {
                logError('Pipeline failed')
                sendSlackNotification([
                    color: 'danger',
                    message: "âŒ Ansible Airgap setup failed for ${env.JOB_NAME} #${env.BUILD_NUMBER}"
                ])
            }
        }

        unstable {
            script {
                logWarning('Pipeline completed with warnings')
                sendSlackNotification([
                    color: 'warning',
                    message: "âš ï¸ Ansible Airgap setup completed with warnings for ${env.JOB_NAME} #${env.BUILD_NUMBER}"
                ])
            }
        }
    }
}

/**
 * HELPER FUNCTIONS
 * These functions are adapted from both Jenkinsfile.recurring and Jenkinsfile.airgap.rke2.improved
 */

def getShortJobName() {
    def jobName = "${env.JOB_NAME}"
    if (jobName.contains('/')) {
        def lastSlashIndex = jobName.lastIndexOf('/')
        return jobName.substring(lastSlashIndex + 1)
    }
    return jobName
}

def validateParameters() {
    // Validate required parameters
    if (!params.RKE2_VERSION) {
        error('RKE2_VERSION parameter is required')
    }
    if (!params.RANCHER_VERSION) {
        error('RANCHER_VERSION parameter is required')
    }
    if (!params.RANCHER_TEST_REPO_URL) {
        error('RANCHER_TEST_REPO_URL parameter is required')
    }
    if (!params.QA_INFRA_REPO_URL) {
        error('QA_INFRA_REPO_URL parameter is required')
    }

    logInfo('Parameters validated successfully')
}

def setupDynamicEnvironment() {
    env.RKE2_VERSION = params.RKE2_VERSION
    env.RANCHER_VERSION = params.RANCHER_VERSION

    logInfo('Dynamic environment configured')
    logInfo("RKE2 Version: ${env.RKE2_VERSION}")
    logInfo("Rancher Version: ${env.RANCHER_VERSION}")
}

def getCredentialsList() {
    return [
        string(credentialsId: 'AWS_ACCESS_KEY_ID', variable: 'AWS_ACCESS_KEY_ID'),
        string(credentialsId: 'AWS_SECRET_ACCESS_KEY', variable: 'AWS_SECRET_ACCESS_KEY'),
        string(credentialsId: 'AWS_SSH_PEM_KEY', variable: 'AWS_SSH_PEM_KEY'),
        string(credentialsId: 'AWS_SSH_KEY_NAME', variable: 'AWS_SSH_KEY_NAME'),
        string(credentialsId: 'SLACK_WEBHOOK', variable: 'SLACK_WEBHOOK')
    ]
}

def generateConfigurationFiles() {
    logInfo('Generating configuration files for infrastructure deployment')

    // Validate required environment variables
    validateConfigurationEnvironmentVariables()

    // Generate Terraform configuration
    generateTerraformConfiguration()

    // Generate Ansible configuration
    generateAnsibleConfiguration()

    // Generate environment file for containers
    generateEnvironmentFile()

    logInfo('Configuration files generated successfully')
}

def validateConfigurationEnvironmentVariables() {
    logInfo('Validating configuration environment variables')

    def requiredVars = [
        'TERRAFORM_CONFIG': 'Terraform configuration content',
        'ANSIBLE_CONFIG': 'Ansible configuration content',
        'RKE2_VERSION': 'RKE2 version for deployment',
        'RANCHER_VERSION': 'Rancher version for deployment'
    ]

    def missingVars = []
    requiredVars.each { varName, description ->
        def varValue = env."${varName}"
        if (!varValue || varValue.trim().isEmpty()) {
            missingVars.add("${varName} (${description})")
        }
    }

    if (!missingVars.isEmpty()) {
        def errorMsg = "Missing required environment variables for configuration:\n- ${missingVars.join('\n- ')}"
        logError(errorMsg)
        throw new IllegalArgumentException(errorMsg)
    }

    logInfo('All required environment variables validated successfully')
}

def generateTerraformConfiguration() {
    logInfo('Generating Terraform configuration')

    if (!env.TERRAFORM_CONFIG) {
        throw new RuntimeException('TERRAFORM_CONFIG environment variable is not set')
    }

    def tofuConfig = env.TERRAFORM_CONFIG

    // Replace variables in config (similar to Jenkinsfile.recurring pattern)
    tofuConfig = tofuConfig.replace('${AWS_SECRET_ACCESS_KEY}', env.AWS_SECRET_ACCESS_KEY)
    tofuConfig = tofuConfig.replace('${AWS_ACCESS_KEY_ID}', env.AWS_ACCESS_KEY_ID)
    tofuConfig = tofuConfig.replace('${AWS_REGION}', env.AWS_REGION)
    tofuConfig = tofuConfig.replace('${AWS_IAM_PROFILE}', env.AWS_IAM_PROFILE ?: '')
    tofuConfig = tofuConfig.replace('${AWS_VPC}', env.AWS_VPC ?: '')
    tofuConfig = tofuConfig.replace('${AWS_SECURITY_GROUPS}', env.AWS_SECURITY_GROUPS ?: '')

    // Write the configuration file
    dir('./qa-infra-automation') {
        dir('./tofu/aws/modules/airgap') {
            writeFile file: env.TERRAFORM_VARS_FILENAME, text: tofuConfig
            logInfo("Terraform configuration written to: ${env.TERRAFORM_VARS_FILENAME}")
        }
    }
}

def generateAnsibleConfiguration() {
    logInfo('Generating Ansible configuration')

    if (!env.ANSIBLE_CONFIG) {
        throw new RuntimeException('ANSIBLE_CONFIG environment variable is not set')
    }

    def ansibleConfig = env.ANSIBLE_CONFIG

    // Replace variables in config
    ansibleConfig = ansibleConfig.replace('${ADMIN_PASSWORD}', env.ADMIN_PASSWORD ?: '')

    // Write the Ansible configuration file to the correct location for airgap deployment
    dir('./qa-infra-automation') {
        // Ensure the directory structure exists
        sh 'mkdir -p ansible/rke2/airgap/inventory/group_vars'

        // Write the group_vars/all.yml file
        writeFile file: 'ansible/rke2/airgap/inventory/group_vars/all.yml', text: ansibleConfig
        logInfo('Ansible configuration written to: ansible/rke2/airgap/inventory/group_vars/all.yml')

        // Also create a copy in the original location for backward compatibility
        sh 'mkdir -p ansible'
        writeFile file: "ansible/${env.ANSIBLE_VARS_FILENAME}", text: ansibleConfig
        logInfo("Ansible configuration also written to: ansible/${env.ANSIBLE_VARS_FILENAME} (backward compatibility)")
    }
}

def generateEnvironmentFile() {
    logInfo('Generating environment file for container execution')

    def envContent = """# Environment variables for infrastructure deployment containers
TF_WORKSPACE=${env.TF_WORKSPACE}
BUILD_NUMBER=${env.BUILD_NUMBER}
JOB_NAME=${env.JOB_NAME}
TERRAFORM_TIMEOUT=${env.TERRAFORM_TIMEOUT}
ANSIBLE_TIMEOUT=${env.ANSIBLE_TIMEOUT}
QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH}
TERRAFORM_VARS_FILENAME=${env.TERRAFORM_VARS_FILENAME}
ANSIBLE_VARS_FILENAME=${env.ANSIBLE_VARS_FILENAME}
RKE2_VERSION=${env.RKE2_VERSION}
RANCHER_VERSION=${env.RANCHER_VERSION}
PRIVATE_REGISTRY_URL=${env.PRIVATE_REGISTRY_URL}
PRIVATE_REGISTRY_USERNAME=${env.PRIVATE_REGISTRY_USERNAME}
"""

    writeFile file: env.ENV_FILE, text: envContent
    logInfo("Environment file created: ${env.ENV_FILE}")
}

def setupSSHKeys() {
    if (env.AWS_SSH_PEM_KEY && env.AWS_SSH_KEY_NAME) {
        logInfo('Setting up SSH keys')

        dir('./tests/.ssh') {
            def decodedKey = new String(env.AWS_SSH_PEM_KEY.decodeBase64())
            writeFile file: env.AWS_SSH_KEY_NAME, text: decodedKey
            sh "chmod 600 ${env.AWS_SSH_KEY_NAME}"
        }

        logInfo('SSH keys configured successfully')
    }
}

def buildDockerImage() {
    logInfo("Building Docker image: ${env.IMAGE_NAME}")

    dir('./') {
        sh './tests/validation/configure.sh'
        sh """
            docker build . \
                -f ./tests/validation/Dockerfile.tofu.e2e \
                -t ${env.IMAGE_NAME} \
                --build-arg BUILD_DATE=\$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
                --build-arg VCS_REF=\$(git rev-parse --short HEAD) \
                --label "pipeline.build.number=${env.BUILD_NUMBER}" \
                --label "pipeline.job.name=${env.JOB_NAME}"
        """
    }

    logInfo('Docker image built successfully')
}

def createSharedVolume() {
    logInfo("Creating shared volume: ${env.VALIDATION_VOLUME}")
    sh "docker volume create --name ${env.VALIDATION_VOLUME}"
}

def executeInContainer(commands) {
    def commandString = commands.join(' && ')
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-${timestamp}"

    // Write the command to a temporary file to avoid shell escaping issues
    def scriptFile = "docker-script-${timestamp}.sh"
    writeFile file: scriptFile, text: commandString

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TF_WORKSPACE} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    // Clean up the temporary script file
    sh "rm -f ${scriptFile}"
}

def executeScriptInContainer(scriptContent, extraEnv = [:]) {
    def timestamp = System.currentTimeMillis()
    def containerName = "${env.BUILD_CONTAINER_NAME}-script-${timestamp}"
    def scriptFile = "docker-script-${timestamp}.sh"

    writeFile file: scriptFile, text: scriptContent

    def envVars = ''
    extraEnv.each { key, value ->
        envVars += " -e ${key}=${value}"
    }

    sh """
        docker run --rm \
            -v ${env.VALIDATION_VOLUME}:/root \
            -v ${pwd()}/${scriptFile}:/tmp/script.sh \
            --name ${containerName} \
            -t --env-file ${env.ENV_FILE} \
            -e QA_INFRA_WORK_PATH=${env.QA_INFRA_WORK_PATH} \
            -e TF_WORKSPACE=${env.TF_WORKSPACE} \
            ${envVars} \
            ${env.IMAGE_NAME} \
            sh /tmp/script.sh
    """

    sh "rm -f ${scriptFile}"
}

def getAnsibleVerbosity() {
    switch (params.LOG_LEVEL) {
        case 'DEBUG':
            return '-vvv'
        case 'VERBOSE':
            return '-vvvv'
        default:
            return '-v'
    }
}

def destroyInfrastructure() {
    logInfo('Destroying infrastructure using S3 backend state')

    try {
        executeScriptInContainer("""
cd ${env.QA_INFRA_WORK_PATH}

echo 'Configuring S3 backend for destruction...'
cat > tofu/aws/modules/airgap/backend.tf << 'EOF'
terraform {
  backend "s3" {
    bucket         = "${env.TF_STATE_BUCKET}"
    key            = "${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate"
    region         = "${env.TF_STATE_REGION}"
    encrypt        = true
    dynamodb_table = "${env.TF_STATE_BUCKET}-locks"
  }
}
EOF

echo 'Initializing backend to access remote state...'
tofu -chdir=tofu/aws/modules/airgap init -input=false -reconfigure

echo 'Selecting workspace: ${env.TF_WORKSPACE}'
tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TF_WORKSPACE} || echo 'Workspace selection failed, continuing...'

echo 'Refreshing state from S3...'
tofu -chdir=tofu/aws/modules/airgap refresh -var-file=${env.TERRAFORM_VARS_FILENAME} || echo 'Refresh failed, continuing...'

echo 'Displaying current state resources...'
tofu -chdir=tofu/aws/modules/airgap state list

echo 'Destroying infrastructure...'
export TF_WORKSPACE=${env.TF_WORKSPACE}
tofu -chdir=tofu/aws/modules/airgap destroy -auto-approve -var-file=${env.TERRAFORM_VARS_FILENAME}

echo 'Verifying destruction...'
REMAINING_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
if [ "\$REMAINING_RESOURCES" -eq 0 ]; then
    echo 'SUCCESS: All resources destroyed'
else
    echo "WARNING: \$REMAINING_RESOURCES resources still in state"
    tofu -chdir=tofu/aws/modules/airgap state list
fi

echo 'Infrastructure destruction completed'
""")
    } catch (Exception e) {
        logError("Infrastructure destruction failed: ${e.message}")
        logError("Manual cleanup may be required for workspace: ${env.TF_WORKSPACE}")
        logError("State location: s3://${env.TF_STATE_BUCKET}/${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate")
    }
}

def destroyInfrastructureOnFailure(String stageName) {
    if (params.DESTROY_ON_FAILURE.toBoolean()) {
        logWarning("DESTROY_ON_FAILURE enabled - Destroying infrastructure due to ${stageName} failure")
        try {
            destroyInfrastructure()
            logWarning("Infrastructure destroyed due to ${stageName} failure")
        } catch (Exception e) {
            logError("Failed to destroy infrastructure during failure cleanup: ${e.message}")
            logError("Manual cleanup may be required for workspace: ${env.TF_WORKSPACE}")
        }
    } else {
        logWarning('DESTROY_ON_FAILURE disabled - Infrastructure will be preserved for debugging')
        logWarning("Manual cleanup will be required for workspace: ${env.TF_WORKSPACE}")
    }
}

def cleanupContainersAndVolumes() {
    logInfo('Cleaning up Docker containers and volumes')

    try {
        if (env.NODE_NAME) {
            sh """
                # Stop and remove any containers with our naming pattern
                docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker stop || true
                docker ps -aq --filter "name=${env.BUILD_CONTAINER_NAME}" | xargs -r docker rm -v || true

                # Remove the Docker image
                docker rmi -f ${env.IMAGE_NAME} || true

                # Remove the shared volume
                docker volume rm -f ${env.VALIDATION_VOLUME} || true

                # Clean up any dangling images and volumes
                docker system prune -f || true
            """
        } else {
            logWarning('No node context available for Docker cleanup')
        }
    } catch (Exception e) {
        logError("Docker cleanup failed: ${e.message}")
    }
}

def archiveBuildArtifacts(artifacts) {
    try {
        archiveArtifacts artifacts: artifacts.join(','), allowEmptyArchive: true
        logInfo("Artifacts archived: ${artifacts.join(', ')}")
    } catch (Exception e) {
        logError("Failed to archive artifacts: ${e.message}")
    }
}

def sendSlackNotification(config) {
    if (env.SLACK_WEBHOOK) {
        try {
            def payload = [
                channel: '#rancher-qa',
                username: 'Jenkins',
                color: config.color,
                title: 'Ansible Airgap Setup Pipeline',
                message: config.message,
                fields: [
                    [title: 'Job', value: env.JOB_NAME, short: true],
                    [title: 'Build', value: env.BUILD_NUMBER, short: true],
                    [title: 'RKE2 Version', value: env.RKE2_VERSION, short: true],
                    [title: 'Rancher Version', value: env.RANCHER_VERSION, short: true]
                ]
            ]

            httpRequest(
                httpMode: 'POST',
                url: env.SLACK_WEBHOOK,
                contentType: 'APPLICATION_JSON',
                requestBody: groovy.json.JsonOutput.toJson(payload)
            )

            logInfo('Slack notification sent successfully')
        } catch (Exception e) {
            logError("Failed to send Slack notification: ${e.message}")
        }
    }
}

/**
 * INFRASTRUCTURE MANAGEMENT FUNCTIONS
 * These are adapted from Jenkinsfile.airgap.rke2.improved
 */

def validateRequiredVariables(requiredVars) {
    logInfo('Validating required environment variables')

    def missingVars = []
    requiredVars.each { varName ->
        def varValue = env."${varName}"
        if (!varValue || varValue.trim().isEmpty()) {
            missingVars.add(varName)
        }
    }

    if (!missingVars.isEmpty()) {
        def errorMsg = "Missing required environment variables: ${missingVars.join(', ')}"
        logError(errorMsg)
        throw new IllegalArgumentException(errorMsg)
    }

    logInfo('All required variables validated successfully')
}

def validateInfrastructurePrerequisites() {
    logInfo('Validating infrastructure prerequisites')

    def prerequisiteScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Checking OpenTofu installation...'
tofu version

echo 'Checking workspace directory...'
test -d ${env.QA_INFRA_WORK_PATH}

echo 'Validating terraform vars file...'
test -f ${env.QA_INFRA_WORK_PATH}/tofu/aws/modules/airgap/${env.TERRAFORM_VARS_FILENAME}

echo 'All infrastructure prerequisites validated successfully'
"""

    try {
        executeScriptInContainer(prerequisiteScript)
        logInfo('All infrastructure prerequisites validated')
    } catch (Exception e) {
        def errorMsg = "Infrastructure prerequisites validation failed: ${e.message}"
        logError(errorMsg)
        throw new RuntimeException(errorMsg, e)
    }
}

def deployInfrastructure() {
    logInfo('Starting infrastructure deployment process')

    // Step 1: Initialize OpenTofu
    initializeOpenTofu()

    // Step 2: Manage workspace
    manageWorkspace()

    // Step 3: Plan infrastructure changes
    planInfrastructure()

    // Step 4: Apply infrastructure
    applyInfrastructure()

    logInfo('Infrastructure deployment completed successfully')
}

def initializeOpenTofu() {
    logInfo('Initializing OpenTofu with S3 backend')

    def initScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Configuring S3 backend for state storage...'
cat > tofu/aws/modules/airgap/backend.tf << 'EOF'
terraform {
  backend "s3" {
    bucket         = "${env.TF_STATE_BUCKET}"
    key            = "${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate"
    region         = "${env.TF_STATE_REGION}"
    encrypt        = true
    dynamodb_table = "${env.TF_STATE_BUCKET}-locks"
  }
}
EOF

echo 'Backend configuration written:'
cat tofu/aws/modules/airgap/backend.tf

echo 'Initializing OpenTofu backend...'
tofu -chdir=tofu/aws/modules/airgap init -input=false -upgrade -reconfigure

echo 'Verifying initialization success...'
tofu -chdir=tofu/aws/modules/airgap providers

echo 'OpenTofu initialization with S3 backend completed successfully'
"""

    executeScriptInContainer(initScript)
}

def manageWorkspace() {
    logInfo("Managing OpenTofu workspace: ${env.TF_WORKSPACE}")

    def workspaceScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Managing workspace state...'
unset TF_WORKSPACE

echo 'Current workspaces:'
tofu -chdir=tofu/aws/modules/airgap workspace list

echo 'Creating or selecting workspace: ${env.TF_WORKSPACE}'
if ! tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TF_WORKSPACE} 2>/dev/null; then
    echo 'Workspace does not exist, creating new workspace...'
    tofu -chdir=tofu/aws/modules/airgap workspace new ${env.TF_WORKSPACE}

    if ! tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TF_WORKSPACE}; then
        echo 'ERROR: Failed to create and select workspace'
        exit 1
    fi
fi

# Verify workspace selection
CURRENT_WORKSPACE=\$(tofu -chdir=tofu/aws/modules/airgap workspace show)
echo "Current workspace: \$CURRENT_WORKSPACE"

# Strip whitespace and handle empty responses
CURRENT_WORKSPACE=\$(echo "\$CURRENT_WORKSPACE" | xargs)

if [ "\$CURRENT_WORKSPACE" = "" ]; then
    echo 'ERROR: Workspace show command returned empty response'
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

if [ "\$CURRENT_WORKSPACE" != "${env.TF_WORKSPACE}" ]; then
    echo "ERROR: Expected workspace ${env.TF_WORKSPACE}, but got '\$CURRENT_WORKSPACE'"
    echo 'Available workspaces:'
    tofu -chdir=tofu/aws/modules/airgap workspace list
    exit 1
fi

export TF_WORKSPACE=${env.TF_WORKSPACE}
echo "Workspace management completed: \$TF_WORKSPACE"
"""

    executeScriptInContainer(workspaceScript)
}

def planInfrastructure() {
    logInfo('Planning infrastructure changes')

    def planScript = """
cd ${env.QA_INFRA_WORK_PATH}
export TF_WORKSPACE=${env.TF_WORKSPACE}

echo 'Generating infrastructure plan for validation...'
tofu -chdir=tofu/aws/modules/airgap plan -input=false -var-file=${env.TERRAFORM_VARS_FILENAME} -out=tfplan

echo 'Checking if plan file was generated in the correct location...'
if [ ! -f tofu/aws/modules/airgap/tfplan ]; then
    echo 'ERROR: Plan file was not generated successfully in module directory'
    echo 'Contents of tofu/aws/modules/airgap/:'
    ls -la tofu/aws/modules/airgap/
    exit 1
fi

echo 'Verifying plan file is not empty...'
PLAN_SIZE=\$(stat -c%s tofu/aws/modules/airgap/tfplan 2>/dev/null || echo 0)
if [ "\$PLAN_SIZE" = "0" ]; then
    echo 'ERROR: Plan file is empty'
    exit 1
fi

echo "Plan file generated successfully (\$PLAN_SIZE bytes) in tofu/aws/modules/airgap/tfplan"

# Copy plan file from module directory to shared volume for persistence
cp tofu/aws/modules/airgap/tfplan /root/tfplan-backup
echo 'Plan file backed up to shared volume'

echo 'Infrastructure plan validation completed'
"""

    executeScriptInContainer(planScript)
}

def applyInfrastructure() {
    logInfo('Applying infrastructure configuration')

    def applyScript = """
cd ${env.QA_INFRA_WORK_PATH}
export TF_WORKSPACE=${env.TF_WORKSPACE}

echo 'Restoring plan file from shared volume...'
if [ -f /root/tfplan-backup ]; then
    # Restore plan file to the correct module directory
    cp /root/tfplan-backup tofu/aws/modules/airgap/tfplan
    echo 'Plan file restored from shared volume to module directory'
else
    echo 'ERROR: No backup plan file found in shared volume'
    echo 'Generating new plan...'
    tofu -chdir=tofu/aws/modules/airgap plan -input=false -var-file=${env.TERRAFORM_VARS_FILENAME} -out=tfplan
fi

# Check if plan was restored/generated successfully in module directory
if [ ! -f tofu/aws/modules/airgap/tfplan ]; then
    echo 'ERROR: Plan file was not generated successfully in module directory'
    exit 1
fi

# Verify the plan file is not empty
PLAN_SIZE=\$(stat -c%s tofu/aws/modules/airgap/tfplan 2>/dev/null || echo 0)
if [ "\$PLAN_SIZE" = "0" ]; then
    echo 'ERROR: Plan file is empty'
    exit 1
fi

echo 'Plan file restored successfully (\$PLAN_SIZE bytes), applying...'
tofu -chdir=tofu/aws/modules/airgap apply -auto-approve -input=false tfplan

# Clean up the plan file after successful application
rm -f tofu/aws/modules/airgap/tfplan

echo 'Verifying state after apply...'
tofu -chdir=tofu/aws/modules/airgap state list

echo 'Generating outputs for downstream stages...'
tofu -chdir=tofu/aws/modules/airgap output -json > ${env.QA_INFRA_WORK_PATH}/infrastructure-outputs.json

echo 'Verifying inventory file generation...'
if [ -f ansible/rke2/airgap/inventory/inventory.yml ] && [ -s ansible/rke2/airgap/inventory/inventory.yml ]; then
    echo 'SUCCESS: inventory.yml generated by tofu apply exists and has content'
    echo 'Fixing YAML syntax issues in inventory file...'

    # Debug: Show the full inventory.yml content first
    echo "=== Full inventory.yml content before fixing ==="
    cat ansible/rke2/airgap/inventory/inventory.yml
    echo "=== End inventory.yml content ==="

    # Show specific lines that might have YAML syntax issues
    echo "=== Lines with potential YAML syntax issues ==="
    grep -n ":" ansible/rke2/airgap/inventory/inventory.yml \\| grep ": [^']" \\| head -10
    echo "=== End potential issues ==="

    # Fix the ProxyCommand quoting issue with a comprehensive approach
    # Create a backup first
    cp ansible/rke2/airgap/inventory/inventory.yml ansible/rke2/airgap/inventory/inventory.yml.backup

    # Use python to properly fix the YAML syntax (more reliable than sed)
    python3 << 'EOF'
import re
import sys

# Read the inventory file
with open('ansible/rke2/airgap/inventory/inventory.yml', 'r') as f:
    content = f.read()

print("=== Python YAML fixer ===")
print("Original content:")
print(content)

# Find the exact problematic line and fix it
# The specific issue is: ProxyCommand='ssh -W %h:%p -i {{ ssh_private_key_file }} {{ bastion_user }}@{{ bastion_host }}'
# We need to change the single quotes to double quotes

# Pattern to match the exact problematic line
pattern = "ProxyCommand='ssh -W %h:%p -i {{ ssh_private_key_file }} {{ bastion_user }}@{{ bastion_host }}'"
replacement = 'ProxyCommand="ssh -W %h:%p -i {{ ssh_private_key_file }} {{ bastion_user }}@{{ bastion_host }}"'

fixed_content = re.sub(pattern, replacement, content)

print("=== After first replacement ===")
print(fixed_content)

# Also handle a more general pattern
pattern2 = r"ProxyCommand='([^']*)'"
replacement2 = r'ProxyCommand="\1"'
fixed_content = re.sub(pattern2, replacement2, fixed_content)

print("=== Final fixed content ===")
print(fixed_content)

# Write the fixed content back
with open('ansible/rke2/airgap/inventory/inventory.yml', 'w') as f:
    f.write(fixed_content)

print("=== Python YAML fixer completed ===")

# Validate the fix
try:
    import yaml
    with open('ansible/rke2/airgap/inventory/inventory.yml', 'r') as f:
        yaml.safe_load(f)
    print("=== YAML validation: SUCCESS ===")
except Exception as e:
    print("=== YAML validation: FAILED ===")
    print(f"Error: {e}")
EOF

    echo 'Fixed inventory file contents:'
    head -30 ansible/rke2/airgap/inventory/inventory.yml

    # Validate the YAML syntax
    echo "=== Validating YAML syntax ==="
    python3 -c "import yaml; yaml.safe_load(open('ansible/rke2/airgap/inventory/inventory.yml', 'r'))" && echo "YAML syntax is valid" || echo "YAML syntax still has issues"
    echo "=== End YAML validation ==="
else
    echo 'ERROR: inventory.yml missing or empty after tofu apply'
    echo 'Contents of ansible/rke2/airgap/inventory/:'
    ls -la ansible/rke2/airgap/inventory/
    exit 1
fi

echo 'Infrastructure provisioned and inventory generated successfully'
"""

    executeScriptInContainer(applyScript)
}

def validateInfrastructureState() {
    logInfo('Validating infrastructure deployment state')

    def validationScript = """
cd ${env.QA_INFRA_WORK_PATH}
export TF_WORKSPACE=${env.TF_WORKSPACE}

echo 'Validating infrastructure state...'
STATE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
echo "Number of resources in state: \$STATE_COUNT"

echo 'Checking critical outputs...'
tofu -chdir=tofu/aws/modules/airgap output -raw vpc_id || echo 'WARNING: vpc_id output missing'
tofu -chdir=tofu/aws/modules/airgap output -raw subnet_ids || echo 'WARNING: subnet_ids output missing'

echo 'Validating inventory files...'
if [ -f ansible/rke2/airgap/inventory/inventory.yml ] && [ -s ansible/rke2/airgap/inventory/inventory.yml ]; then
    echo 'SUCCESS: inventory.yml exists and has content'
else
    echo 'WARNING: inventory.yml missing or empty'
fi

echo 'Infrastructure state validation completed'
"""

    executeScriptInContainer(validationScript)
}

def archiveValidationResults() {
    logInfo('Archiving validation results')

    try {
        // Copy artifacts from container
        sh """
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/ansible/rke2/kubeconfig.yaml ./kubeconfig.yaml || true
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/tofu/aws/modules/airgap/terraform.tfstate ./terraform.tfstate || true
        """

        // Generate deployment summary
        def deploymentSummary = [
            buildNumber: env.BUILD_NUMBER,
            rke2Version: env.RKE2_VERSION,
            rancherVersion: env.RANCHER_VERSION,
            timestamp: new Date().format("yyyy-MM-dd'T'HH:mm:ss'Z'"),
            status: currentBuild.currentResult ?: 'SUCCESS'
        ]

        writeFile file: 'deployment-summary.json', text: groovy.json.JsonOutput.toJson(deploymentSummary)
    } catch (Exception e) {
        logError("Failed to archive validation results: ${e.message}")
    }
}

def handleInfrastructureFailure(failureType, exception) {
    logError("Handling infrastructure failure of type: ${failureType}")

    try {
        switch (failureType) {
            case 'TIMEOUT':
                logError('Infrastructure deployment timed out')
                archiveInfrastructureState()
                break
            case 'DEPLOYMENT_FAILED':
                logError('Infrastructure deployment failed')
                safeDestroyInfrastructure()
                break
            default:
                logError("Unknown failure type: ${failureType}")
                safeDestroyInfrastructure()
        }

        archiveFailureDiagnostics(exception)
    } catch (Exception cleanupException) {
        logError("Cleanup failed during failure handling: ${cleanupException.message}")
    }
}

def safeDestroyInfrastructure() {
    logInfo('Attempting safe infrastructure destruction')

    try {
        def destroyCommands = [
            "cd ${env.QA_INFRA_WORK_PATH}",
            "export TF_WORKSPACE=${env.TF_WORKSPACE}",
            "echo 'Selecting workspace for destruction...'",
            "tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TF_WORKSPACE} || echo 'Workspace selection failed'",
            "echo 'Destroying infrastructure...'",
            "tofu -chdir=tofu/aws/modules/airgap destroy -auto-approve -var-file=${env.TERRAFORM_VARS_FILENAME} || echo 'Destroy completed with warnings'",
            "echo 'Infrastructure destruction completed'"
        ]

        timeout(time: 15, unit: 'MINUTES') {
            executeInContainer(destroyCommands)
        }

        logInfo('Infrastructure destroyed successfully')
    } catch (Exception e) {
        logError("Infrastructure destruction failed: ${e.message}")
        logError("Manual cleanup may be required for workspace: ${env.TF_WORKSPACE}")
    }
}

def archiveInfrastructureState() {
    logInfo('Archiving infrastructure state to S3 and Jenkins artifacts')

    try {
        def archiveScript = """
cd ${env.QA_INFRA_WORK_PATH}
export TF_WORKSPACE=${env.TF_WORKSPACE}

echo 'Pulling latest state from S3 backend...'
tofu -chdir=tofu/aws/modules/airgap refresh || echo 'Refresh failed, continuing with existing state'

echo 'Exporting state and outputs...'
tofu -chdir=tofu/aws/modules/airgap show -no-color > infrastructure-state-summary.txt 2>/dev/null || echo 'Could not generate state summary'
tofu -chdir=tofu/aws/modules/airgap output -json > infrastructure-outputs.json 2>/dev/null || echo 'Could not generate outputs'
tofu -chdir=tofu/aws/modules/airgap state pull > infrastructure-${env.TF_WORKSPACE}.tfstate 2>/dev/null || echo 'Could not pull state'

echo 'Creating state metadata...'
cat > infrastructure-state-metadata.json << EOF
{
    "workspace": "${env.TF_WORKSPACE}",
    "build_number": "${env.BUILD_NUMBER}",
    "job_name": "${env.JOB_NAME}",
    "timestamp": "\$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "s3_bucket": "${env.TF_STATE_BUCKET}",
    "s3_key": "${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate",
    "region": "${env.TF_STATE_REGION}"
}
EOF

echo 'Verifying archived files:'
ls -la infrastructure-${env.TF_WORKSPACE}.tfstate infrastructure-outputs.json infrastructure-state-metadata.json 2>/dev/null

echo 'State archival to local files completed'
"""

        executeScriptInContainer(archiveScript)

        // Archive files from shared volume to Jenkins workspace
        sh """
            # Copy files from shared volume to Jenkins workspace
            docker run --rm \
                -v ${env.VALIDATION_VOLUME}:/root \
                -v ${pwd()}:/workspace \
                --name archival-check-${env.BUILD_NUMBER} \
                -t ${env.IMAGE_NAME} \
                sh -c "
                    echo 'Copying state files to Jenkins workspace...'
                    cp /root/go/src/github.com/rancher/qa-infra-automation/infrastructure-${env.TF_WORKSPACE}.tfstate /workspace/ 2>/dev/null || echo 'Failed to copy tfstate'
                    cp /root/go/src/github.com/rancher/qa-infra-automation/infrastructure-outputs.json /workspace/ 2>/dev/null || echo 'Failed to copy outputs'
                    cp /root/go/src/github.com/rancher/qa-infra-automation/infrastructure-state-summary.txt /workspace/ 2>/dev/null || echo 'Failed to copy summary'
                    cp /root/go/src/github.com/rancher/qa-infra-automation/infrastructure-state-metadata.json /workspace/ 2>/dev/null || echo 'Failed to copy metadata'
                    echo 'Files in workspace:'
                    ls -la /workspace/infrastructure-*
                "
        """

        // Archive as Jenkins artifacts for easy retrieval
        archiveArtifacts artifacts: 'infrastructure-*.tfstate,infrastructure-outputs.json,infrastructure-state-metadata.json,infrastructure-state-summary.txt', allowEmptyArchive: true
        
        logInfo('State archived to both S3 backend and Jenkins artifacts')
    } catch (Exception e) {
        logError("Failed to archive infrastructure state: ${e.message}")
    }
}

def archiveInfrastructureFailureArtifacts() {
    logInfo('Archiving infrastructure failure artifacts')

    try {
        def debugCommands = [
            "cd ${env.QA_INFRA_WORK_PATH}",
            "export TF_WORKSPACE=${env.TF_WORKSPACE}",
            'tofu -chdir=tofu/aws/modules/airgap workspace list > workspace-list.txt 2>&1',
            "tofu -chdir=tofu/aws/modules/airgap state list > state-resources.txt 2>&1 || echo 'No state available'",
            "tofu -chdir=tofu/aws/modules/airgap providers > providers.txt 2>&1 || echo 'Providers not available'",
            // "aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags[?Key==\\`Name\\`].Value|[0]]' --output table > aws-instances.txt 2>&1 || echo 'Could not query AWS instances'",
            "echo 'Failure artifact collection completed'"
        ]

        executeInContainer(debugCommands)

        // Copy debug files from container
        sh """
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/workspace-list.txt ./ || true
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/state-resources.txt ./ || true
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/providers.txt ./ || true
            docker cp \\\$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/aws-instances.txt ./ || true
        """
    } catch (Exception e) {
        logError("Failed to archive failure artifacts: ${e.message}")
    }
}

def archiveFailureDiagnostics(exception) {
    logInfo('Archiving failure diagnostics')

    def diagnostics = [
        timestamp: new Date().format("yyyy-MM-dd'T'HH:mm:ss'Z'"),
        buildNumber: env.BUILD_NUMBER,
        workspace: env.TF_WORKSPACE,
        failureType: exception.class.simpleName,
        errorMessage: exception.message,
        stackTrace: getStackTracePreview(exception.stackTrace, 10)
    ]

    try {
        writeFile file: 'infrastructure-failure-diagnostics.json',
                  text: groovy.json.JsonOutput.prettyPrint(groovy.json.JsonOutput.toJson(diagnostics))

        logInfo('Failure diagnostics archived successfully')
    } catch (Exception e) {
        logError("Failed to archive failure diagnostics: ${e.message}")
    }
}

def getStackTracePreview(stackTrace, maxLines) {
    if (!stackTrace) {
        return 'No stack trace available'
    }

    def lines = []
    def count = 0

    for (element in stackTrace) {
        if (count >= maxLines) {
            break
        }
        lines.add(element.toString())
        count++
    }

    return lines.join('\n')
}

/**
 * LOGGING FUNCTIONS
 */

def logInfo(message) {
    echo "â„¹ï¸ [INFO] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logError(message) {
    echo "âŒ [ERROR] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logWarning(message) {
    echo "âš ï¸ [WARNING] ${new Date().format('HH:mm:ss')} - ${message}"
}

def logDebug(message) {
    if (params.LOG_LEVEL == 'DEBUG' || params.LOG_LEVEL == 'VERBOSE') {
        echo "ðŸ” [DEBUG] ${new Date().format('HH:mm:ss')} - ${message}"
    }
}

/**
 * STATE MANAGEMENT FUNCTIONS
 * Enhanced functions for Terraform state storage and retrieval
 */

def retrieveStateFromS3() {
    logInfo("Retrieving Terraform state from S3 for workspace: ${env.TF_WORKSPACE}")

    try {
        def retrieveScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Configuring S3 backend for state retrieval...'
cat > tofu/aws/modules/airgap/backend.tf << 'EOF'
terraform {
  backend "s3" {
    bucket         = "${env.TF_STATE_BUCKET}"
    key            = "${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate"
    region         = "${env.TF_STATE_REGION}"
    encrypt        = true
    dynamodb_table = "${env.TF_STATE_BUCKET}-locks"
  }
}
EOF

echo 'Initializing backend...'
tofu -chdir=tofu/aws/modules/airgap init -input=false -reconfigure

echo 'Selecting workspace...'
tofu -chdir=tofu/aws/modules/airgap workspace select ${env.TF_WORKSPACE} || {
    echo 'Workspace does not exist in backend, creating...'
    tofu -chdir=tofu/aws/modules/airgap workspace new ${env.TF_WORKSPACE}
}

echo 'Pulling current state...'
tofu -chdir=tofu/aws/modules/airgap state pull > current-state.tfstate

echo 'Verifying state contents...'
STATE_SIZE=\$(stat -c%s current-state.tfstate 2>/dev/null || echo 0)
if [ "\$STATE_SIZE" -gt 0 ]; then
    echo "SUCCESS: State retrieved (\$STATE_SIZE bytes)"
    RESOURCE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
    echo "Resources in state: \$RESOURCE_COUNT"
else
    echo 'WARNING: State is empty or not found'
fi

echo 'State retrieval completed'
"""

        executeScriptInContainer(retrieveScript)
        logInfo('State successfully retrieved from S3 backend')
        return true
    } catch (Exception e) {
        logError("Failed to retrieve state from S3: ${e.message}")
        return false
    }
}

def retrieveStateFromJenkinsArtifacts(buildNumber = null) {
    logInfo("Retrieving Terraform state from Jenkins artifacts")

    try {
        def targetBuild = buildNumber ?: env.BUILD_NUMBER
        def artifactPattern = "infrastructure-${env.TF_WORKSPACE}.tfstate"
        
        // Check if artifacts exist in current build
        if (fileExists(artifactPattern)) {
            logInfo("State file found in current build artifacts")
            return true
        }

        // For destruction jobs, this would typically copy from upstream job
        logWarning("State retrieval from Jenkins artifacts requires manual intervention")
        logWarning("Expected artifact: ${artifactPattern}")
        logWarning("Ensure the state file is available in the workspace before destruction")
        
        return false
    } catch (Exception e) {
        logError("Failed to retrieve state from Jenkins artifacts: ${e.message}")
        return false
    }
}

def validateStateConsistency() {
    logInfo('Validating Terraform state consistency')

    try {
        def validationScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Checking state integrity...'
tofu -chdir=tofu/aws/modules/airgap validate

echo 'Comparing local and remote state...'
tofu -chdir=tofu/aws/modules/airgap plan -detailed-exitcode -var-file=${env.TERRAFORM_VARS_FILENAME} || {
    EXIT_CODE=\$?
    if [ \$EXIT_CODE -eq 2 ]; then
        echo 'WARNING: State drift detected - infrastructure differs from state'
    elif [ \$EXIT_CODE -eq 1 ]; then
        echo 'ERROR: Plan failed due to configuration issues'
        exit 1
    else
        echo 'SUCCESS: No drift detected'
    fi
}

echo 'State consistency validation completed'
"""

        executeScriptInContainer(validationScript)
        logInfo('State consistency validated')
    } catch (Exception e) {
        logWarning("State consistency validation failed: ${e.message}")
        logWarning("Manual state inspection may be required")
    }
}

def backupStateToMultipleLocations() {
    logInfo('Creating redundant state backups')

    try {
        def backupScript = """
cd ${env.QA_INFRA_WORK_PATH}

TIMESTAMP=\$(date -u +%Y%m%d_%H%M%S)
BACKUP_DIR="state-backups/\$TIMESTAMP"
mkdir -p "\$BACKUP_DIR"

echo 'Creating state backup...'
tofu -chdir=tofu/aws/modules/airgap state pull > "\$BACKUP_DIR/terraform.tfstate"

echo 'Creating state metadata...'
cat > "\$BACKUP_DIR/metadata.json" << EOF
{
    "workspace": "${env.TF_WORKSPACE}",
    "backup_timestamp": "\$TIMESTAMP",
    "build_number": "${env.BUILD_NUMBER}",
    "job_name": "${env.JOB_NAME}",
    "s3_bucket": "${env.TF_STATE_BUCKET}",
    "s3_key": "${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate"
}
EOF

echo 'Backup created in:' "\$BACKUP_DIR"
ls -la "\$BACKUP_DIR"
"""

        executeScriptInContainer(backupScript)
        
        // Archive backup to Jenkins
        sh """
            docker cp \$(docker ps -aqf "name=${env.BUILD_CONTAINER_NAME}"):${env.QA_INFRA_WORK_PATH}/state-backups ./state-backups || true
        """
        
        archiveArtifacts artifacts: 'state-backups/**/*', allowEmptyArchive: true
        
        logInfo('State backup completed and archived')
    } catch (Exception e) {
        logError("Failed to create state backup: ${e.message}")
    }

/**
 * ROLLBACK AND RECOVERY FUNCTIONS
 * Enhanced functions for infrastructure rollback and state recovery
 */

def performEmergencyRollback() {
    logError('Performing emergency rollback of infrastructure')

    try {
        // Create backup of current state before rollback
        backupStateToMultipleLocations()

        def rollbackScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Emergency rollback initiated...'
echo "Target workspace: ${env.TF_WORKSPACE}"
echo "Timestamp: \$(date -u +%Y-%m-%dT%H:%M:%SZ)"

# Try to destroy infrastructure safely
echo 'Attempting safe infrastructure destruction...'
tofu -chdir=tofu/aws/modules/airgap destroy -auto-approve -var-file=${env.TERRAFORM_VARS_FILENAME} || {
    echo 'Normal destroy failed, attempting force destroy...'
    
    # Force destroy individual resources if bulk destroy fails
    tofu -chdir=tofu/aws/modules/airgap state list | while read resource; do
        echo "Force destroying resource: \$resource"
        tofu -chdir=tofu/aws/modules/airgap destroy -target="\$resource" -auto-approve -var-file=${env.TERRAFORM_VARS_FILENAME} || echo "Failed to destroy \$resource"
    done
}

# Verify destruction
REMAINING_RESOURCES=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
if [ "\$REMAINING_RESOURCES" -eq 0 ]; then
    echo 'SUCCESS: Emergency rollback completed - all resources destroyed'
else
    echo "WARNING: \$REMAINING_RESOURCES resources remain after rollback"
    tofu -chdir=tofu/aws/modules/airgap state list
fi

echo 'Emergency rollback completed'
"""

        executeScriptInContainer(rollbackScript)
        logInfo('Emergency rollback completed')
        
        // Clean up workspace
        cleanupWorkspaceAfterRollback()
        
    } catch (Exception e) {
        logError("Emergency rollback failed: ${e.message}")
        logError("Manual intervention required for workspace: ${env.TF_WORKSPACE}")
        logError("State location: s3://${env.TF_STATE_BUCKET}/${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate")
    }
}

def restoreStateFromBackup(backupTimestamp) {
    logInfo("Restoring state from backup: ${backupTimestamp}")

    try {
        def restoreScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Restoring state from backup...'
BACKUP_FILE="state-backups/${backupTimestamp}/terraform.tfstate"

if [ -f "\$BACKUP_FILE" ]; then
    echo 'Backup file found, restoring...'
    
    # Push the backup state to remote backend
    tofu -chdir=tofu/aws/modules/airgap state push "\$BACKUP_FILE"
    
    echo 'State restored from backup'
    
    # Verify restoration
    RESOURCE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
    echo "Resources restored: \$RESOURCE_COUNT"
    
else
    echo 'ERROR: Backup file not found'
    exit 1
fi
"""

        executeScriptInContainer(restoreScript)
        logInfo('State restoration completed')
        
    } catch (Exception e) {
        logError("State restoration failed: ${e.message}")
        throw new RuntimeException("Failed to restore state from backup", e)
    }
}

def cleanupWorkspaceAfterRollback() {
    logInfo('Cleaning up workspace after rollback')

    try {
        def cleanupScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'Post-rollback cleanup...'

# Switch to default workspace
echo 'Switching to default workspace...'
tofu -chdir=tofu/aws/modules/airgap workspace select default || echo 'Could not switch to default workspace'

# Delete the failed workspace
echo 'Deleting failed workspace...'
tofu -chdir=tofu/aws/modules/airgap workspace delete ${env.TF_WORKSPACE} || echo 'Could not delete workspace'

# List remaining workspaces
echo 'Remaining workspaces:'
tofu -chdir=tofu/aws/modules/airgap workspace list

echo 'Workspace cleanup completed'
"""

        executeScriptInContainer(cleanupScript)
        logInfo('Workspace cleanup completed')
        
    } catch (Exception e) {
        logWarning("Workspace cleanup failed: ${e.message}")
    }
}

def performStateHealthCheck() {
    logInfo('Performing state health check')

    try {
        def healthCheckScript = """
cd ${env.QA_INFRA_WORK_PATH}

echo 'State health check initiated...'

# Check state integrity
echo 'Checking state file integrity...'
tofu -chdir=tofu/aws/modules/airgap validate

# Check for state drift
echo 'Checking for infrastructure drift...'
tofu -chdir=tofu/aws/modules/airgap plan -detailed-exitcode -var-file=${env.TERRAFORM_VARS_FILENAME} > drift-check.txt 2>&1
DRIFT_EXIT_CODE=\$?

case \$DRIFT_EXIT_CODE in
    0)
        echo 'SUCCESS: No drift detected'
        ;;
    1)
        echo 'ERROR: Configuration issues detected'
        cat drift-check.txt
        exit 1
        ;;
    2)
        echo 'WARNING: Infrastructure drift detected'
        cat drift-check.txt
        ;;
esac

# Check resource accessibility
echo 'Verifying resource accessibility...'
RESOURCE_COUNT=\$(tofu -chdir=tofu/aws/modules/airgap state list | wc -l)
echo "Total resources in state: \$RESOURCE_COUNT"

if [ "\$RESOURCE_COUNT" -eq 0 ]; then
    echo 'WARNING: No resources found in state'
else
    echo 'State health check passed'
fi
"""

        executeScriptInContainer(healthCheckScript)
        logInfo('State health check completed')
        return true
        
    } catch (Exception e) {
        logError("State health check failed: ${e.message}")
        return false
    }
}

def createRollbackPlan() {
    logInfo('Creating rollback plan')

    try {
        def rollbackData = [
            workspace: env.TF_WORKSPACE,
            timestamp: new Date().format("yyyy-MM-dd'T'HH:mm:ss'Z'"),
            buildNumber: env.BUILD_NUMBER,
            jobName: env.JOB_NAME,
            s3StateLocation: "s3://${env.TF_STATE_BUCKET}/${env.TF_STATE_KEY_PREFIX}/${env.TF_WORKSPACE}/terraform.tfstate",
            rollbackInstructions: [
                "1. Use destruction pipeline with TARGET_WORKSPACE=${env.TF_WORKSPACE}",
                "2. Set STATE_SOURCE=S3_BACKEND for primary state retrieval",
                "3. Set STATE_SOURCE=JENKINS_ARTIFACTS as fallback with BUILD_NUMBER=${env.BUILD_NUMBER}",
                "4. Enable DRY_RUN=true for initial validation",
                "5. Set FORCE_DESTROY=true only if normal process fails"
            ],
            emergencyContacts: [
                "DevOps Team: #rancher-devops",
                "Infrastructure Team: #rancher-infrastructure"
            ]
        ]

        writeFile file: 'rollback-plan.json', text: groovy.json.JsonOutput.prettyPrint(groovy.json.JsonOutput.toJson(rollbackData))
        
        // Archive rollback plan
        archiveArtifacts artifacts: 'rollback-plan.json', allowEmptyArchive: true
        
        logInfo('Rollback plan created and archived')
        
    } catch (Exception e) {
        logError("Failed to create rollback plan: ${e.message}")
    }
}

def performGracefulShutdown() {
    logInfo('Performing graceful infrastructure shutdown')

    try {
        // Create final state backup before shutdown
        backupStateToMultipleLocations()
        
        // Archive all important artifacts
        archiveInfrastructureState()
        
        // Create rollback plan for potential restoration
        createRollbackPlan()
        
        // Optional: Perform controlled destruction if DESTROY_ON_FAILURE is enabled
        if (params.DESTROY_ON_FAILURE?.toBoolean()) {
            logInfo('DESTROY_ON_FAILURE enabled - performing controlled destruction')
            destroyInfrastructure()
        } else {
            logInfo('Infrastructure preserved for manual cleanup')
            logInfo("Use destruction pipeline with workspace: ${env.TF_WORKSPACE}")
        }
        
    } catch (Exception e) {
        logError("Graceful shutdown failed: ${e.message}")
        // Attempt emergency procedures
        performEmergencyRollback()
    }
}
}
